[
  {
    "objectID": "Publications.html",
    "href": "Publications.html",
    "title": "Publications",
    "section": "",
    "text": "Information extracted from Google Scholar Profile\nGoogle Scholar Citations = 353  H-Index = 11  i10-Index = 13 \nInformation and definitions of various indices"
  },
  {
    "objectID": "Publications.html#publication-metrics",
    "href": "Publications.html#publication-metrics",
    "title": "Publications",
    "section": "",
    "text": "Information extracted from Google Scholar Profile\nGoogle Scholar Citations = 353  H-Index = 11  i10-Index = 13 \nInformation and definitions of various indices"
  },
  {
    "objectID": "Publications.html#books",
    "href": "Publications.html#books",
    "title": "Publications",
    "section": "Books",
    "text": "Books"
  },
  {
    "objectID": "Publications.html#peer-review-literature",
    "href": "Publications.html#peer-review-literature",
    "title": "Publications",
    "section": "Peer Review Literature",
    "text": "Peer Review Literature\nCarey, J., K. Jankowski, P. Julian, L. Sethna, P. Thomas, and J. J. Rohweder. 2019. Exploring Silica Stoichiometry on a Large Floodplain Riverscape. Frontiers in Ecology and Evolution 7: 346. Husk, B., P. Julian, N. Tromas, D. Phan, K. Painter, H. M. Baulch, and S. Sauvé. 2023. Improving Water Quality in a Hypereutrophic Lake and Tributary Through Agricultural Nutrient Mitigation: A Multi-Year Monitoring Analysis.doi:10.2139/ssrn.4572355 Jankowski, K. J., K. Johnson, L. Sethna, and others. 2023. Long-Term Changes in Concentration and Yield of Riverine Dissolved Silicon From the Poles to the Tropics. Global Biogeochemical Cycles 37: e2022GB007678. doi:10.1029/2022GB007678 Julian, P. 2011. Home range dynamics of female Florida panthers in reponse to kitten production. Florida Scientist 74. Julian, P. 2013a. Comment on Spatial and temporal phosphorus distribution changes in a large wetland ecosystem by X. Zapata-Rios et al.: Commentary. Water Resources Research 49: 2312–2313. doi:10.1002/wrcr.20162 Julian, P. 2013b. Mercury Bio-concentration Factor in Mosquito Fish (Gambusia spp.) in the Florida Everglades. Bulletin of Environmental Contamination and Toxicology 90: 329–332. doi:10.1007/s00128-012-0939-6 Julian, P. 2013c. Mercury hotspot identification in Water Conservation Area 3, Florida, USA. Annals of GIS 19: 79–88. doi:10.1080/19475683.2013.782469 Julian, P. 2014. Reply to Mercury Bioaccumulation and Bioaccumulation Factors for Everglades Mosquitofish as Related to Sulfate: A Re-Analysis of Julian II (2013). Bulletin of Environmental Contamination and Toxicology 93: 517–521. doi:10.1007/s00128-014-1389-0 Julian, P. 2015. South Florida Coastal Sediment Ecological Risk Assessment. Bulletin of environmental contamination and toxicology 95: 188–193. Julian, P. 2016. Commentary on Mitsch et al., 2015, Protecting the Florida Everglades wetlands with wetlands: Can stormwater phosphorus be reduced to oligotrophic conditions? Ecological Engineering 108: 333–337. Julian, P. 2017a. Assessment of Upper Taylor Slough water quality and implications for ecosystem management in Everglades National Park. Wetlands Ecology and Management 25: 191–209. doi:10.1007/s11273-016-9509-8 Julian, P. 2017b. Letter to the Editor regarding Surratt D, Shindle D, Yongshan W, et al. Letter to the Editor regarding: Julian P, 2017. Assessment of Upper Taylor Slough water quality and implications for ecosystem management in Everglades National Park. Wetlands Ecol Manage 1–3. doi:10.1007/s11273-017-9571-x Julian, P. 2019. Spatial Ecology and Conservation Modeling. Applications with R. Robert Fletcher, Marie-Josee Fortin: Book Review. Austral Ecology. doi:10.1111/aec.12791 Julian, P. 2020. Getting the science right to protect and restore our environment. A critique of Lapointe et al. (2019) Nitrogen enrichment, altered stoichiometry, and coral reef decline at Looe Key, Florida Keys, USA: a 3-decade study. Mar Biol 167: 68. doi:10.1007/s00227-020-3667-1 Julian, P., R. Chambers, and T. Russell. 2017a. Iron and Pyritization in Wetland Soils of the Florida Coastal Everglades. Estuaries and Coasts 40: 822–831. doi:10.1007/s12237-016-0180-3 Julian, P., and M. W. Cunningham. 2013. Total mercury concentration in Florida black bear (Ursus americanus floridanus). Florida Scientist 76. Julian, P., E. M. Everham III, and M. B. Main. 2012. Influence of a Large-scale Removal of an Invasive Plant (Melaleuca quinquenervia) on Home-range Size and Habitat Selection by Female Florida Panthers (Puma concolor coryi) within Big Cypress National Preserve, Florida. Southeastern Naturalist 11: 337–348. Julian, P., J. Fourqurean, S. Davis, and others. Submitted. Long-term spatiotemporal patterns and trends in water quality reveal a coastal continuum of disturbance legacies. Limnology and Oceanography. doi:https://doi.org/10.21203/rs.3.rs-1753636/v1 Julian, P., S. Gerber, R. K. Bhomia, J. King, T. Z. Osborne, and A. L. Wright. 2020. Understanding stoichiometric mechanisms of nutrient retention in wetland macrophytes: stoichiometric homeostasis along a nutrient gradient in a subtropical wetland. Oecologia. doi:10.1007/s00442-020-04722-9 Julian, P., S. Gerber, R. K. Bhomia, J. King, T. Z. Osborne, A. L. Wright, M. Powers, and J. Dombrowski. 2019. Evaluation of nutrient stoichiometric relationships among ecosystem compartments of a subtropical treatment wetland. Do we have “Redfield wetlands”? Ecol Process 8: 20. doi:10.1186/s13717-019-0172-x Julian, P., S. Gerber, A. L. Wright, B. Gu, and T. Z. Osborne. 2017b. Carbon pool trends and dynamics within a subtropical peatland during long-term restoration. Ecol Process 6: 43–57. doi:10.1186/s13717-017-0110-8 Julian, P., and B. Gu. 2015. Mercury accumulation in largemouth bass (Micropterus salmoides Lacépède) within marsh ecosystems of the Florida Everglades, USA. Ecotoxicology 24: 202–214. doi:10.1007/s10646-014-1373-9 Julian, P., B. Gu, and G. Redfield. 2015. Comment on and Reinterpretation of Gabriel et al. (2014) Fish Mercury and Surface Water Sulfate Relationships in the Everglades Protection Area. Environmental Management 55: 1–5. doi:10.1007/s00267-014-0377-9 Julian, P., B. Gu, and A. L. Wright. 2016a. Mercury Stoichiometric Relationships in a Subtropical Peatland. Water Air Soil Pollut 227: 472. doi:10.1007/s11270-016-3180-9 Julian, P., and T. Z. Osborne. 2018. From lake to estuary, the tale of two waters: A study of aquatic continuum biogeochemistry. Environment Monitoring and Assessment 190: 1–24. doi:https://doi.org/10.1007/s10661-017-6455-8 Julian, P., T. Z. Osborne, R. K. Bhomia, and O. Villapando. 2021. Knowing your limits: evaluating aquatic metabolism in a subtropical treatment wetland. Hydrobiologia. doi:10.1007/s10750-021-04617-7 Julian, P., T. Z. Osborne, and R. Ellis. 2023a. Evaluation of Biogeochemical Changes in Channelized and Restored Portions of a Subtropical Floodplain. Hydrobiology 2: 1–18. doi:10.3390/hydrobiology2010001 Julian, P., and L. Reidenbach. Submitted. Upstream water management and its role in estuary health, evaluation of freshwater management and subtropical estuary function. Water Resources Management. Julian, P., T. Schafer, M. J. Cohen, P. Jones, and T. Z. Osborne. 2023b. Changes in the spatial distribution of total phosphorus in sediment and water column of a shallow subtropical lake. Lake and Reservoir Management 39: 213–230. Julian, P., M. Thompson, and E. C. Milbrandt. 2024. Dark waters: Evaluating seagrass community response to optical water quality and freshwater discharges in a highly managed subtropical estuary. Regional Studies in Marine Science 69: 103302. doi:10.1016/j.rsma.2023.103302 Julian, P., and Z. Welch. 2022. Understanding the ups and downs, application of hydrologic restoration measures for a large Subtropical Lake. Lake and Reservoir Management 38: 304–317. doi:https://doi.org/10.21203/rs.3.rs-1739423/v2 Julian, P., A. L. Wright, and T. Z. Osborne. 2016b. Iron and sulfur porewater and surface water biogeochemical interactions in subtropical peatlands. Soil Science Society of America Journal 80: 794–802. Kominoski, J. S., E. E. Gaiser, E. Castaneda‐Moya, and others. 2020. Disturbance legacies increase and synchronize nutrient concentrations and bacterial productivity in coastal ecosystems. Ecology 101. doi:https://doi.org/10.1002/ecy.2988 Marazzi, L., C. M. Finlayson, P. A. Gell, P. Julian, J. S. Kominoski, and E. E. Gaiser. 2018. Balancing wetland restoration benefits to people and nature. Solutions Journal 9. Schafer, T. B., P. Julian, O. Villapando, and T. Z. Osborne. 2023. Abiotic mineralization of dissolved organic phosphorus for improved nutrient retention in a large-scale treatment wetland system. Ecological Engineering 195: 107078. doi:10.1016/j.ecoleng.2023.107078 Schafer, T., N. Ward, P. Julian, K. R. Reddy, and T. Z. Osborne. 2020. Impacts of Hurricane Disturbance on Water Quality across the Aquatic Continuum of a Blackwater River to Estuary Complex. Journal of Marine Science and Engineering 8: 412. doi:10.3390/jmse8060412 Smith, M. C., P. Julian, D. DeAngelis, and B. Zhang. 2023. Ecological benefits of integrative weed management of Melaleuca quinquenervia in Big Cypress National Preserve. BioControl. doi:10.1007/s10526-023-10229-y"
  },
  {
    "objectID": "Publications.html#technical-literature",
    "href": "Publications.html#technical-literature",
    "title": "Publications",
    "section": "Technical Literature",
    "text": "Technical Literature"
  },
  {
    "objectID": "news/20241026_Timeseries1.html",
    "href": "news/20241026_Timeseries1.html",
    "title": "Time series Analysis (Part I), the basics",
    "section": "",
    "text": "This blog post effectively breaks my blog writing dry streak. The last offical blog post (not stats related) was on my old blog platform (link) 1170 days ago! This post was motivated by a recent LinkedIn post by Joachim Schork about breaking down time series (see below and here) and the comments from the larger community. Its also motivated by a recent spur of time series decomposition analyses I’ve seen of late during meetings and discussions with colleagues."
  },
  {
    "objectID": "news/20241026_Timeseries1.html#definition",
    "href": "news/20241026_Timeseries1.html#definition",
    "title": "Time series Analysis (Part I), the basics",
    "section": "Definition",
    "text": "Definition\nFirst, what is time series data? If you look in the dictionary it will say something along the lines of a series of values/measurements/observations obtained at successive times, often (but not always) with equal intervals between them. Simply put its data collected over time. In my corner of science that could mean daily water levels, weekly total phosphorus concentrations, annual seagrass coverage, etc. Once collected these data can analyzed in a variety of ways depending on the motivation of why and where its being collected. Again, in my realm of the science, something we are interested in is the change in conditions overtime for a variety of reasons including (but not limited to) climate change, landscape scale changes (i.e. land-use alterations, dam removal, stream diversion, etc.), restoration activities, forecast modeling, etc. In this case, a time series analysis is needed to see how things are changing overtime."
  },
  {
    "objectID": "news/20241026_Timeseries1.html#formatting",
    "href": "news/20241026_Timeseries1.html#formatting",
    "title": "Time series Analysis (Part I), the basics",
    "section": "Formatting",
    "text": "Formatting\nWhen handling time series data in R you can either handle the data as a data.frame (or tibble if you are a tidyverse person … I personally am not, no judgement) or a ts object.\nGenerate some example value\n\nset.seed(123)\nvalues &lt;- runif(5)|&gt;\n  round(2)\n\n\n\n\ndata.frame\n\n\nts\n\n\n\n\ndata.frame(Yr = 2012:2016, \n           value = values\n           )\n\n    Yr value\n1 2012  0.29\n2 2013  0.79\n3 2014  0.41\n4 2015  0.88\n5 2016  0.94\n\n\n\nts(values, start = 2012)\n\nTime Series:\nStart = 2012 \nEnd = 2016 \nFrequency = 1 \n[1] 0.29 0.79 0.41 0.88 0.94\n\n\n\n\nThere are pros and cons of formatting the data either way. Most time series analysis functions can handle both in someway but most like it to be a ts object. The way the ts function works is it essentially converts the data into a special kind of data matrix (in R its called class or object) with an added header containing some information about the time series (like the example above). Depending on the information you include in the ts function it makes some assumptions, for instance with frequency = 12 it assumes its monthly data or frequency = 365 assumes daily.\n\nrnorm(24)|&gt;\n  round(2)|&gt;\n  ts(frequency = 12)\n\n    Jan   Feb   Mar   Apr   May   Jun   Jul   Aug   Sep   Oct   Nov   Dec\n1 -1.69  1.24 -0.11 -0.12  0.18  1.28 -1.73  1.69  0.50  2.53  0.55  0.24\n2 -1.05  1.29  0.83 -0.06 -0.78 -0.73 -0.22 -0.33 -1.09 -0.09  1.07 -0.15\n\n\nSee ?ts for more details on specifics. A very big drawback when using ts is if you are working with daily data with a leap year or years with leap years mixed in ts doesn’t know how to handle that extra day since the input is a vector or list of data. There might be a way to coerce it but I’ve yet to figure it out. Which is why I prefer to work in the data.frame world whenever possible."
  },
  {
    "objectID": "news/20241026_Timeseries1.html#synthetic-data",
    "href": "news/20241026_Timeseries1.html#synthetic-data",
    "title": "Time series Analysis (Part I), the basics",
    "section": "Synthetic Data",
    "text": "Synthetic Data\nAs an example this post (and the following series) rather than grabbing some random datasets as an example I wanted to use something that we have some knowledge on before digging into the specifics. This includes some of the components discussed above. To achieve this I put together some basic functions to simulate some data. The first function simulate_timeseries is relatively basic and simulate_timeseries_vol is a little more complex that includes some volatility and randomness factors.\n\nsimulate_timeseries &lt;- function(n = 1000,            # Number of observations\n                                trend_slope = 0.01,    # Slope of linear trend\n                                seasonal_amp = 2,      # Amplitude of seasonal component\n                                seasonal_period = 12,  # Period length\n                                noise_mean = 1,        # mean of noise\n                                noise_sd = 0.5,        # Standard deviation of noise  \n                                seed.val = 123         # value to set.seed(...)\n){         \n  if(is.null(seed.val)==F){set.seed(seed.val)} # for reproducibility\n  # Generate time points\n  t &lt;- 1:n\n  # Create components\n  # 1. Linear trend\n  trend &lt;- trend_slope * t\n  # 2. Seasonal component using sine wave\n  seasonal &lt;- seasonal_amp * sin(2 * pi * t / seasonal_period)\n  # 3. Random noise (stationary component)\n  noise &lt;- rnorm(n, mean = noise_mean, sd = noise_sd)\n  # Combine components\n  ts_data &lt;- trend + seasonal + noise\n  \n  # Convert to time series object\n  ts_result &lt;- ts(ts_data, frequency = seasonal_period)\n  # Return both the time series and its components for analysis\n  return(list(\n    timeseries = data.frame(time = t, value = as.numeric(ts_result)),\n    components = list(\n      trend = trend,\n      seasonal = seasonal,\n      noise = noise\n    )\n  ))\n}\n\nsimulate_timeseries_vol &lt;- function(n = 1000,              # Number of observations\n                                    trend_slope = 0.01,    # Slope of linear trend\n                                    seasonal_amp = 2,      # seasonal component\n                                    seasonal_period = 12,  # Period length\n                                    init_vol = 0.5,        # Initial volatility\n                                    vol_persistence = 0.95,# Persistence in volatility\n                                    rw_sd = 0.1,           # Random walk innovation SD\n                                    seed.val = 123         # value to set.seed(...)  \n){         \n  if(is.null(seed.val)==F){set.seed(seed.val)} # for reproducibility\n  # Generate time points\n  t &lt;- 1:n\n  # 1. Create non-linear trend (combining linear trend with random walk)\n  linear_trend &lt;- trend_slope * t\n  random_walk &lt;- cumsum(rnorm(n, 0, rw_sd))\n  trend &lt;- linear_trend + random_walk\n  # 2. Create time-varying seasonal component\n  # Amplitude changes over time following a random walk\n  varying_amplitude &lt;- seasonal_amp + cumsum(rnorm(n, 0, rw_sd/5))\n  seasonal &lt;- varying_amplitude * sin(2 * pi * t / seasonal_period)\n  # 3. Generate time-varying volatility (GARCH-like process)\n  # Initialize volatility vector\n  volatility &lt;- numeric(n)\n  volatility[1] &lt;- init_vol\n  # Generate volatility process\n  for(i in 2:n) {\n    # Volatility follows AR(1) process with innovations\n    volatility[i] &lt;- sqrt(0.01 + \n                            vol_persistence * volatility[i-1]^2 + \n                            (1 - vol_persistence) * rnorm(1, 0, 0.1)^2)\n  }\n  # 4. Generate heteroskedastic noise\n  noise &lt;- rnorm(n, 0, 1) * volatility\n  # 5. Add structural breaks\n  # Add random level shifts\n  n_breaks &lt;- max(1, round(n/200))  # Approximately one break every 200 observations\n  break_points &lt;- sort(sample(2:n, n_breaks))\n  level_shifts &lt;- cumsum(rnorm(n_breaks, 0, 2))  # Random shift magnitudes\n  breaks &lt;- numeric(n)\n  current_break &lt;- 1\n  for(i in 1:n) {\n    if(current_break &lt;= length(break_points) && i &gt;= break_points[current_break]) {\n      breaks[i:n] &lt;- level_shifts[current_break]\n      current_break &lt;- current_break + 1\n    }\n  }\n  \n  # Combine all components\n  ts_data &lt;- trend + seasonal + noise + breaks\n  # Convert to time series object\n  ts_result &lt;- ts(ts_data, frequency = seasonal_period)\n  # Return both the time series and its components\n  return(list(\n    timeseries = data.frame(time = t, value = as.numeric(ts_result)),\n    components = list(\n      trend = trend,\n      seasonal = seasonal,\n      noise = noise,\n      breaks = breaks,\n      volatility = volatility\n    )\n  ))\n}\n\nHere are some example simulated datasets. Lets assume this data is monthly for a 15 year period of record.\n\nn.vals &lt;- 15*12 # 15 years 12 months per year \ndat1 &lt;- simulate_timeseries(\n  n = n.vals,           # About 15 years of monthly data\n  trend_slope = 0.025,  # Upward trend\n  seasonal_amp = 2,     # Seasonal fluctuation of ±2\n  seasonal_period = 12, # Monthly seasonality\n  noise_sd = 0.5,       # Moderate noise\n  noise_mean = 3\n)\n## add some years and months\ndat1$timeseries  &lt;-  cbind(dat1$timeseries,\n                           expand.grid(Mon = 1:12, Yr = 1990:2004)\n)\n\ndat2 &lt;- simulate_timeseries_vol(\n  n = n.vals,           # About 15 years of daily data\n  trend_slope = 0.05,   # Upward trend\n  seasonal_amp = 2,     # Base seasonal amplitude\n  seasonal_period = 10, # Monthly seasonality\n  init_vol = 0.5,      # Initial volatility\n  vol_persistence = 0.65,\n  rw_sd = 0.3\n)\n## add some years and months\ndat2$timeseries  &lt;-  cbind(dat2$timeseries,\n                           expand.grid(Mon = 1:12, Yr = 1990:2004)\n)\n\ndat3 &lt;- simulate_timeseries(\n  n = n.vals,           # About 15 years of monthly data\n  trend_slope = 0.001,  # no trend\n  seasonal_amp = 2,     # Seasonal fluctuation of ±2\n  seasonal_period = 12, # Monthly seasonality\n  noise_sd = 0.5,       # Moderate noise\n  noise_mean = 3\n)\n\ndat4 &lt;- simulate_timeseries(\n  n = n.vals,           # About 15 years of monthly data\n  trend_slope = -0.05,  # downward trend\n  seasonal_amp = 2,     # Seasonal fluctuation of ±2\n  seasonal_period = 12, # Monthly seasonality\n  noise_sd = 0.5,       # Moderate noise\n  noise_mean = 15\n)\n\nHere is a quick plot of the data …\n\n\n\n\n\nQuick plots of example data"
  },
  {
    "objectID": "news/20241026_Timeseries1.html#basic-trend-analysis",
    "href": "news/20241026_Timeseries1.html#basic-trend-analysis",
    "title": "Time series Analysis (Part I), the basics",
    "section": "Basic Trend Analysis",
    "text": "Basic Trend Analysis\nA simple trend test would look like this … For simplicity sake we are going to use time as its numeric but hypothetically decimal date (see lubridate::decimal_date for more info) is also an option.\n\ncor.test(dat1$timeseries$time,dat1$timeseries$value,method = \"kendall\")\n\n\n    Kendall's rank correlation tau\n\ndata:  dat1$timeseries$time and dat1$timeseries$value\nz = 8.7492, p-value &lt; 2.2e-16\nalternative hypothesis: true tau is not equal to 0\nsample estimates:\n     tau \n0.438982 \n\ncor.test(dat2$timeseries$time,dat2$timeseries$value,method = \"kendall\")\n\n\n    Kendall's rank correlation tau\n\ndata:  dat2$timeseries$time and dat2$timeseries$value\nz = 11.996, p-value &lt; 2.2e-16\nalternative hypothesis: true tau is not equal to 0\nsample estimates:\n      tau \n0.6018622 \n\n\nBased on the plots you would have guessed that the datasets had an increasing trend. But here is what no-trend (dat3) and negative trend (dat4) looks like.\n\n\n\n\n\nQuick plots of example data\n\n\n\n\n\ncor.test(dat3$timeseries$time,dat3$timeseries$value,method = \"kendall\")\n\n\n    Kendall's rank correlation tau\n\ndata:  dat3$timeseries$time and dat3$timeseries$value\nz = -0.24248, p-value = 0.8084\nalternative hypothesis: true tau is not equal to 0\nsample estimates:\n        tau \n-0.01216636 \n\ncor.test(dat4$timeseries$time,dat4$timeseries$value,method = \"kendall\")\n\n\n    Kendall's rank correlation tau\n\ndata:  dat4$timeseries$time and dat4$timeseries$value\nz = -13.708, p-value &lt; 2.2e-16\nalternative hypothesis: true tau is not equal to 0\nsample estimates:\n       tau \n-0.6877716 \n\n\nSometimes we are more interested in long term trends … be careful when using “long” and “short” these are usually very context specific and care should be taken to delineate/define as much as possible. For sake of brevity lets look at an annual trend analysis.\nFirst, the data needs to be aggregated:\n\ndat1_yr &lt;- aggregate(value~Yr,dat1$timeseries,mean)\n\nYou can also look at the annual variability or standard deviation using aggregate.\n\naggregate(value~Yr,dat1$timeseries,sd)\n\n     Yr    value\n1  1990 1.631108\n2  1991 1.674150\n3  1992 1.277267\n4  1993 1.359000\n5  1994 1.481055\n6  1995 1.197387\n7  1996 1.442100\n8  1997 1.458378\n9  1998 1.646581\n10 1999 1.457668\n11 2000 1.455360\n12 2001 1.447669\n13 2002 1.428132\n14 2003 1.460503\n15 2004 1.533227\n\n\n\n\n\n\n\nQuick plots of aggregated data from dat1.\n\n\n\n\nOr do something more elaborate to calculate all sorts of statistics. My preference (again not tidyverse) is functionality in the plyr package.\n\nlibrary(plyr)\nddply(dat1$timeseries,c(\"Yr\"),summarise,\n      mean.val = mean(value),\n      sd.val = sd(value),\n      var.val = var(value),\n      N.val = length(value), #or AnalystHelper::N.obs(value)\n      SE = AnalystHelper::SE(value),\n      med.val = median(value),\n      min.val = min(value),\n      max.val = max(value))|&gt;\n  round(2)# added to consolidate the table down\n\n     Yr mean.val sd.val var.val N.val   SE med.val min.val max.val\n1  1990     3.26   1.63    2.66    12 0.47    3.61    0.84    5.85\n2  1991     3.36   1.67    2.80    12 0.48    3.03    0.99    6.03\n3  1992     3.85   1.28    1.63    12 0.37    4.20    1.92    6.09\n4  1993     4.02   1.36    1.85    12 0.39    3.96    1.86    5.82\n5  1994     4.48   1.48    2.19    12 0.43    4.82    1.65    6.40\n6  1995     4.60   1.20    1.43    12 0.35    4.42    2.99    6.41\n7  1996     4.96   1.44    2.08    12 0.42    4.88    3.03    7.14\n8  1997     5.44   1.46    2.13    12 0.42    5.46    3.30    7.72\n9  1998     5.48   1.65    2.71    12 0.48    5.27    3.15    7.95\n10 1999     5.75   1.46    2.12    12 0.42    5.66    3.90    7.94\n11 2000     6.20   1.46    2.12    12 0.42    6.26    3.74    8.05\n12 2001     6.29   1.45    2.10    12 0.42    6.56    4.05    8.70\n13 2002     6.70   1.43    2.04    12 0.41    6.46    4.61    8.78\n14 2003     7.18   1.46    2.13    12 0.42    6.97    4.92    9.46\n15 2004     7.42   1.53    2.35    12 0.44    7.79    5.12    9.17\n\n\nOnce the data is aggregated we can use the simple Kendall test to look at the overall annual trend in the data.\n\ncor.test(dat1_yr$Yr,dat1_yr$value,method = \"kendall\")\n\n\n    Kendall's rank correlation tau\n\ndata:  dat1_yr$Yr and dat1_yr$value\nT = 105, p-value = 1.529e-12\nalternative hypothesis: true tau is not equal to 0\nsample estimates:\ntau \n  1 \n\n\nLooks like a perfect increasing trend! The Kendall-𝜏 is a type of correlation coefficient so the values should range between -1 and 1. Based on the 𝜌-value, (assuming an 𝛼-value of 0.05) this is a statistically significant trend. (Lets not get into the 𝜌-value discussion!)\nWe can also look at the rate of change. The most common is the Theil-Sen slope estimator. There are several different packages out there that can estimate Theil-Sen slope but the most basic one that I’ve come across is the median based linear model or mblm package. We can discuss the basics of Theil-Sen and mblm in a later post but after some digging and search what I’ve determined is that Theil-Sen method and median-based linear model are closely related. Generally, the Theil-Sen method is a non-parametric method that calculates the median of slopes of all lines through the data and the intercept is often the median of the residuals after fitting the slope. Meanwhile, the median-based linear model is a regression model that uses medians as a criterion for fitting a model and minimizes the sum of absolute residuals. In most cases, the Theil-Sen slope and the slope determined by the median-based linear model are the same with minuscule differences. Most trend routines use median-based linear models or quantile regressions as a basis to perform Theil-Sen estimation.\nIf you run the side-by-side of mblm, zyp and a separate Theil-Sen estimator pulled from the litature (found here) you’ll undoubtly come to similar, if not the same values. Don’t believe me lets give it a try.\n\nlibrary(zyp)\nlibrary(mblm)\n\n# From University of Virigina function (from literature)\ntheil_sen &lt;- function(x,y){\n  n &lt;- length(x)\n  max_n_slopes &lt;- (n * (n - 1)) / 2\n  slopes &lt;- vector(mode = 'list', length = max_n_slopes) # list of NULL elements\n  add_at &lt;- 1\n  # Calculate point-to-point slopes if x_i != x_j\n  for (i in 1:(n - 1)) {\n    slopes_i &lt;- lapply((i + 1):n, function(j) \n      if (x[j] != x[i]) { (y[j] - y[i]) / (x[j] - x[i]) })\n    slopes[add_at:(add_at + length(slopes_i) - 1)] &lt;- slopes_i\n    add_at &lt;- add_at + length(slopes_i)\n  }\n  # Calculate Theil-Sen slope\n  slopes &lt;- unlist(slopes) # drop NULL elements\n  theil_sen_slope &lt;- median(slopes, na.rm = TRUE)\n  # Calculate Theil-Sen intercept\n  intercepts &lt;- y - (theil_sen_slope * x)\n  theil_sen_intercept &lt;- median(intercepts, na.rm = TRUE)\n  # Return\n  c('Theil-Sen intercept' = theil_sen_intercept, \n    'Theil-Sen slope' = theil_sen_slope)\n}\n\n\nset.seed(123)\n\n# Generate linear data\nx &lt;- 1:100\ny &lt;- 2*x + rnorm(100, 0, 10)\n# Add some outliers\ny[c(10, 30, 50, 70, 90)] &lt;- y[c(10, 30, 50, 70, 90)] + 50\n\nplot(y~x,las=1)\n\n\n\nzyp.sen &lt;-  zyp.sen(y~x); # zyp package\nmblm.sen &lt;-  mblm(y~x,repeated = F); # mblm package\nUV.sen &lt;-  theil_sen(x,y); # University of Virigina function (from literature)\n\n\n\n  Pack intercept   Slope\n1  zyp   0.40106 2.01976\n2 mblm   0.40106 2.01976\n3   UV   0.40106 2.01976\n\n\nThere you have it, all three methods produce the same estimates.\nBack to our aggregated dataset, the code is pretty straight forward but there are a couple of things to be aware. If you’ve fit linear models using lm its the same general format. However, currently the mblm function can’t handle datasets with NAs (unlike lm), so you might have to clean up the data a little (i.e. na.omit() can be your friend). Also double check that both variables are numeric.\n\nlibrary(mblm)\n\ndat1.theilsen &lt;- mblm(value~Yr,dat1_yr,repeated=F)\n\ndat1.theilsen\n\n\nCall:\nmblm(formula = value ~ Yr, dataframe = dat1_yr, repeated = F)\n\nCoefficients:\n(Intercept)           Yr  \n  -587.9833       0.2971  \n\n# extrapolate example\npredict(dat1.theilsen,data.frame(Yr=2005:2014),interval =\"confidence\")\n\n         fit       lwr       upr\n1   7.629230  7.500516  7.757944\n2   7.926294  7.784963  8.067624\n3   8.223357  8.069143  8.377572\n4   8.520421  8.353116  8.687725\n5   8.817484  8.636929  8.998040\n6   9.114548  8.920614  9.308482\n7   9.411611  9.204195  9.619028\n8   9.708675  9.487692  9.929658\n9  10.005739  9.771119 10.240358\n10 10.302802 10.054488 10.551116\n\n\nThe Theil-Sen estimator for the annually aggregated data is 0.297 units per year. This is the rate of annual change. If this was something like sea-level rise, rainfall rate, population growth, etc. you can do a very basic analysis and ask at this rate what would the values be in 10 years (extrapolating … 10.3 units) or in the case of sea-level rise whats the rise per decade (2.97 units per decade)?"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "\nWelcome to the swamp\n",
    "section": "",
    "text": "Welcome to the swamp\n\n\nI am an ecologist with a multi-disciplinary background focused on aquatic biogeochemistry and ecology at the landscape scale. Generally, I can be classified as an aquatic biogeochemist studying the transformation of nutrients and other compounds along the freshwater to marine aquatic continuum. I enjoy studying all aquatic ecosystems but wetlands hold a special place in my heart.\nA couple of words/phrases to describe me: Wetland Biogeochemist, Ecologist, Data-scientist, Soil-scientist, Naturalist, Swamp Walker, Lover of all things R.\nThis website will host some general information about myself, my research interests and publications.\n#rstats #python #geospatial #GIS #soil #ecology #climatechange #biogeochemistry #wetland #aquatic\n\n\n\n\nSelected Publications\nGoogle Scholar Citations = 353  H-Index = 11  i10-Index = 13 \n\nJulian et al (2024) Dark waters: Evaluating seagrass community response to optical water quality and freshwater discharges in a highly managed subtropical estuary. Regional Studies in Marine Science 69:103302.\nJulian et al (2023) Changes in the spatial distribution of total phosphorus in sediment and water column of a shallow subtropical lake. Lake and Reservoir Management 39:213–230.\nJulian et al (2022) Understanding the ups and downs, application of hydrologic restoration measures for a large Subtropical Lake. Lake and Reservoir Management 38:304–317.\nSmith MC, Julian P, DeAngelis D, Zhang B (2023) Ecological benefits of integrative weed management of Melaleuca quinquenervia in Big Cypress National Preserve. BioControl.\n\n\n\n\n\n\nR Packages\nEPGMr   LimnoPalettes  NADA2  AnalystHelper   LORECOVER  CalSalMod \n\n\n\n\n\n\n\n\nWords of inspiration\n\n“My temple is the swamp …When I would recreate myself, I seek the darkest wood, the thickest and most impenetrable and to the citizen, most dismal, swamp. I enter a swamp as a sacred place, a sanctum sanctorum… I seemed to have reached a new world, so wild a place …far away from human society. What’s the need of visiting far-off mountains and bogs, if a half-hour’s walk will carry me into such wildness and novelty.”\nHenry David Thoreau, Walden and Other Writings\n“Whatever you want to do in this world, it is achievable. The most important thing that I’ve found, that perhaps you could use, is be passionate and enthusiastic in the direction that you choose in life, and you’ll be a winner.”\nSteve Irwin, Crikey! What an Adventure\n“…One thing you quickly sense about the people here: they include more than average proportion of idealists and self-motivated seekers. But they’re all pretty down-to-earth. The work demands a lot of physical and mental effort. Slackers don’t make it this far.”\nCarl Safina, Eye of the Albatross\n“I remember a hundred lovely lakes, and recall the fragrant breath of pine and fir and cedar and poplar trees. The trail has strung upon it, as upon a thread of silk, opalescent dawns and saffron sunsets. It has given me blessed release from care and worry and the troubled thinking of our modern day. It has been a return to the primitive and the peaceful. Whenever the pressure of our complex city life thins my blood and benumbs my brain, I seek relief in the trail; and when I hear the coyote wailing to the yellow dawn, my cares fall from me - I am happy.”\nHamlin Garland “Hitting the Trail” - 1899\n\n\n\n\n\n\n\n\nAuthorship Guidelines"
  },
  {
    "objectID": "Authorship.html",
    "href": "Authorship.html",
    "title": "Authorship Agreement",
    "section": "",
    "text": "1Adapted from the Saltmarsh Habitat & Avian Research Program (https://www.tidalmarshbirds.org/) Guidelines for Authorship Standard Operating Procedure.\n\nDeciding authorship on scientific publications can be complicated because practices and cultural norms vary across disciplines and even across labs within the same discipline. This is especially relevant in ecology, where standardized guidelines are lacking and a diversity of options exist for deciding authorship and author order. These guidelines outline a set of criteria for authorship determinations. These criteria are presented as guidelines, because a common set of expectations is important to maintain mutual satisfaction among co- authors. We recognize, however, that some flexibility will be required and communication is essential to the process. Before all else, remember conversations regarding authorship for each manuscript should happen early, frequently, openly, and inclusively. A conversation should be expected when the paper is first conceived and should be revisited periodically as each project develops.\n\n\nAuthorship on a manuscript is warranted when a researcher has made a substantial contribution to the manuscript in question (not the overall project as a whole), as defined by any two of the following:\n\nConceiving of ideas and/or study design and/or analytical approach\nWriting of the manuscript (or sections)\nReviewing and editing the manuscript\nAnalyzing data\nInterpreting results\nCollecting data in field or lab (except in rare circumstances this will not include temporary technicians)\nCreating or managing critical databases (e.g. demographic database, historical abundance estimates spreadsheet)\nObtaining funding (e.g., proposal writing, grant management, and project reports)\n\nOther things are important to keep in mind (expanded upon in the sections below).\n\nA conversation is necessary for each manuscript\nConsider thresholds of effort (“could the study have been done without the contribution?”)\nOffer further involvement\nUse inclusion to deal with uncertainty (“better to be inclusive than to exclude”)\nPrimary authors and their advisors/PIs/mentors will make the final decision on authorship\nInvite prior individuals (alumni) to participate when appropriate\nThere are exceptions for grant deliverables.\nWhen in doubt, talk it out!\n\nConsider Thresholds of Effort\nUltimately, the primary author must have some leeway in making authorship decisions, and ensuring that a certain minimum threshold of contribution has been made. When the level of the contribution to the particular manuscript is unclear (e.g. as in the case of data collection), the deciding question becomes “could the study have been done without that person’s contribution?”\nFor instance, did the extra work amount to a few data points within a huge dataset (if the analysis was enhanced by their participation, but was possible without it, authorship may not be warranted), or were the data points critical to establishing the pattern (if the analysis is impossible without the data from this study site, or if trends depend on those data, authorship is more clearly warranted). Other considerations: If a researcher is collecting data for a study that is not their own, did they do extra work that they would not otherwise have done on their own study site or for their own study (if so, the case for authorship increases)? Did the effort amount to a few days of fieldwork (may not warrant authorship) or a season’s worth of logistics and data collection (more clearly warrants authorship)?\nOffer Further Involvement\nIn some instances, a contributor may have clearly passed a threshold of effort (see previous), but will have only contributed to one of the categories that would qualify them for consideration of authorship. In these instances, it is the primary author’s responsibility to reach out to the contributor early in the writing process and have a conversation about further involvement. The second category can be easily achieved by assistance with developing and reviewing the manuscript, and contributors that have clearly passed a threshold of effort should be given that opportunity.\nUse Inclusion to Deal with Uncertainty\nRecognize that the contribution of effort is a gradient with clear endpoints (one data point out of two probably gets you authorship, one data point out of 1,000 probably doesn’t), so there will likely be situations where it is unclear (200 data points out of 1,000?). If there is any uncertainty in gauging the contribution, it is better to be inclusive than to exclude, and it is better to talk directly to the contributor explicitly. A quick phone call made in a spirit of inclusion can almost always improve the situation for everyone, both for this manuscript and for future collaboration.\nInvite Alumni to Participate when Appropriate\nAs data collected by others who have moved on are used in analysis, we need to give them credit for their prior work. If the data have already been published in another form and their papers can be cited, this may be enough. Authorship may be warranted or offered, however, under several circumstances. First, if the individual was involved in the conception of the ideas in the new manuscript, this would warrant their inclusion. Second, if the new manuscript is based largely on the data (or conceptual groundwork) of a single individual (similar to the rules for contemporary contributors, could the analysis be completed without their data?), authorship should be considered. If the previous work of prior individuals passes the Threshold of Effort test for any reason, the burden is on the PI involved to offer the opportunity for further engagement in the new manuscript to new individuals, preferably early in the process. If prior individuals respond positively and stay involved, then they should be authors on the new work. Prior individuals are responsible for deciding to stay engaged and following through with their involvement. Importantly, prior individuals should understand that if they do not respond to inquiries about authorship, historical datasets can still be used, but they will not be included as authors. This same approach may be followed for the advisors of prior individuals if they pass the Threshold.\n\n\n\nThe order of authors on publications will follow the practice of first-last author emphasis. The first author will be the person who did the majority of the work, carried out the study, and will often be a the primary writer of the manuscript. Typically, the last author should be the lab PI. “Credit” or “importance” is attributed to authors in the following order: first, last, 2nd, 3rd, 4th, etc. Final decisions regarding author order ultimately lie with the first author, in consultation with their advisor, PI or mentor. Authors that disagree with the draft author order, however, should feel comfortable voicing their concerns. More importantly, primary authors should ask specifically for co-authors to approve the final order via email.\nAnyone listed as an author must be given a fair opportunity to read and comment on the manuscript. If prospective authors do not respond in a reasonable amount of time, they should be removed, barring exceptional circumstances (the response could be as simple as “manuscript is good to go”, as long as the author acknowledges and approves the content). The lead author should give AT LEAST two weeks for the response period and should specify a date by which comments are due. Two corollaries of this guideline are 1) anyone has the right to request removal as author from a paper for any reason, including a personal judgment of failure to cross the Threshold for Effort, and 2) no one should ever be an author on a manuscript where they did not approve the final submitted draft (note that many journals have this requirement).\nIt is the primary author’s responsibility, as corresponding author, to provide all co-authors with:\n\nA digital version of the final submitted draft\nNews of all significant correspondence with the editor/publisher\nThe opportunity to assist with revisions\nA digital version of all revisions submitted for publication and the responses to reviewers\nPage proofs and the opportunity to comment on them\nA final pdf of all published papers\n\n\n\n\nExpectations for authorship are a set of evolving cultural norms. This means that 1) they must be taught anew to each set of students and/or new individual early in their involvement with the project and 2) the guidelines in this document need to be revisited and updated regularly (~annually or as needed).\n\n\n\n\n\nThe dissemination of research to the broader scientific community is not only part of the research process, but perhaps the ultimate reason for conducting research in the first place. There is an expectation and responsibility to share one’s work with the broader community (Cooke et al. 2014). It goes without saying that composing a manuscript by committee can be difficult. Co-authorship comes about in a variety of ways some of which may not require a formal role in writing but rather the collection of data, editing of the manuscript, etc. Some manuscripts are planned in advanced and have the luxury of identfying the core author pool. Other manuscripts are produced ad-hoc. Regardless of the origin it is recommended that if the manuscript has more than two authors it is advisable that a lead author along with a supporting person (typically senior author assumes this role) be identified. The objective of having two (or more) authors is to have these individuals generate a paper that is as close to final as possible, after all two-eyes are better than one. Letting other co-authors know that the paper is ready to move forward in the eyes of both the lead senior authors will often help to elicit a rapid review and response by co-authors.\nEveryone is busy, therefore some degree of time-management is needed when composing an manuscript. Therefore it is recommended that tentative deadlines/time-periods are discussed early and often. If necessary these deadline can be revised as needed due to changing priorities within reason but make sure these changes are communicated amongst all authors. Once a draft manuscript is produced and ready for co-author input provide a reasonable review deadline (couple of weeks). Typically, if co-authors are unresponsive its not they don’t find the manuscript important. It is possible that it has been shuffled down the dreaded “to do” list or they just forgot. This has happened to all of us at some point. Therefore a simple reminder would be helpful to re-engage the co-authors. Below is a flow chart from Cooke et al. (2014) with a suggested example of sequence of events to work with tardy or non-responsive co-authors.\n\n\n\n\n\nFlow-diagram with a possile sequence of events for working with tardy or non-responsive co-authors when preparing a manuscript for submission to a journal. From Cooke et al. (2014).\n\n\n\n\nAs suggested above, if co-authors are completely unresponsive it is the lead authors progrative, with input from co-authors to remove unresponsive co-authors. The Committee on Publication Ethics (COPE), has additional guidance information regarding authorship and ethics.\n\n\n\n\nJournals are increasingly requiring authors to specify roles according to author contributions. The days of honorary authorship is slowly (too slowly) coming to an end. However, there are many other aspects of research that are not adequately captured by the generally recognized short list of contributing roles, particularly in applied environmental disciplines such as conservation science, environmental science and applied ecology (herein referred to as conservation science). Cooke et al. (2021) provides some additional context and consideration to authorship guidelines reviewing existing and proposing potential expanded contributors roles.\nBottom line\nPoints summarized from the abstract of Cook et al. (2021).\n\nAuthorship should acknowledge and reward those deserving of credit. Being an author on a paper also means that one assumes some degree of ownership of the content, regardless of the authorship order. (Something, it feels, has been forgotten recently)\nThe growing recognition that authorship should reflect contributions that extend beyond the usual data collection, analysis and writing provides the ideal backdrop for rethinking contributions in conservation science. Cook et al. (2021) propose a more inclusive approach to authorship that recognizes and values diverse contributions and contributors using an expanded list of defined roles.\n\n\n\n\n\n\nCooke SJ, Donaldson MR, Clark TD (2014) Practical guidance for early career researchers dealing with tardy or unresponsive co-authors. Ideas in Ecology and Evolution. 1(7). Link.\nCooke SJ, Nguyen VM, Young N, et al (2021) Contemporary authorship guidelines fail to recognize diverse contributions in conservation science research. Ecological Solutions and Evidence 2:e12060. Link.\nSaltmarsh Habitat & Avian Research Program (2014) Guidelines for authorship. Saltmarsh Habitat & Avian Research Program. https://www.tidalmarshbirds.org/."
  },
  {
    "objectID": "Authorship.html#authorship-criteria",
    "href": "Authorship.html#authorship-criteria",
    "title": "Authorship Agreement",
    "section": "",
    "text": "Authorship on a manuscript is warranted when a researcher has made a substantial contribution to the manuscript in question (not the overall project as a whole), as defined by any two of the following:\n\nConceiving of ideas and/or study design and/or analytical approach\nWriting of the manuscript (or sections)\nReviewing and editing the manuscript\nAnalyzing data\nInterpreting results\nCollecting data in field or lab (except in rare circumstances this will not include temporary technicians)\nCreating or managing critical databases (e.g. demographic database, historical abundance estimates spreadsheet)\nObtaining funding (e.g., proposal writing, grant management, and project reports)\n\nOther things are important to keep in mind (expanded upon in the sections below).\n\nA conversation is necessary for each manuscript\nConsider thresholds of effort (“could the study have been done without the contribution?”)\nOffer further involvement\nUse inclusion to deal with uncertainty (“better to be inclusive than to exclude”)\nPrimary authors and their advisors/PIs/mentors will make the final decision on authorship\nInvite prior individuals (alumni) to participate when appropriate\nThere are exceptions for grant deliverables.\nWhen in doubt, talk it out!\n\nConsider Thresholds of Effort\nUltimately, the primary author must have some leeway in making authorship decisions, and ensuring that a certain minimum threshold of contribution has been made. When the level of the contribution to the particular manuscript is unclear (e.g. as in the case of data collection), the deciding question becomes “could the study have been done without that person’s contribution?”\nFor instance, did the extra work amount to a few data points within a huge dataset (if the analysis was enhanced by their participation, but was possible without it, authorship may not be warranted), or were the data points critical to establishing the pattern (if the analysis is impossible without the data from this study site, or if trends depend on those data, authorship is more clearly warranted). Other considerations: If a researcher is collecting data for a study that is not their own, did they do extra work that they would not otherwise have done on their own study site or for their own study (if so, the case for authorship increases)? Did the effort amount to a few days of fieldwork (may not warrant authorship) or a season’s worth of logistics and data collection (more clearly warrants authorship)?\nOffer Further Involvement\nIn some instances, a contributor may have clearly passed a threshold of effort (see previous), but will have only contributed to one of the categories that would qualify them for consideration of authorship. In these instances, it is the primary author’s responsibility to reach out to the contributor early in the writing process and have a conversation about further involvement. The second category can be easily achieved by assistance with developing and reviewing the manuscript, and contributors that have clearly passed a threshold of effort should be given that opportunity.\nUse Inclusion to Deal with Uncertainty\nRecognize that the contribution of effort is a gradient with clear endpoints (one data point out of two probably gets you authorship, one data point out of 1,000 probably doesn’t), so there will likely be situations where it is unclear (200 data points out of 1,000?). If there is any uncertainty in gauging the contribution, it is better to be inclusive than to exclude, and it is better to talk directly to the contributor explicitly. A quick phone call made in a spirit of inclusion can almost always improve the situation for everyone, both for this manuscript and for future collaboration.\nInvite Alumni to Participate when Appropriate\nAs data collected by others who have moved on are used in analysis, we need to give them credit for their prior work. If the data have already been published in another form and their papers can be cited, this may be enough. Authorship may be warranted or offered, however, under several circumstances. First, if the individual was involved in the conception of the ideas in the new manuscript, this would warrant their inclusion. Second, if the new manuscript is based largely on the data (or conceptual groundwork) of a single individual (similar to the rules for contemporary contributors, could the analysis be completed without their data?), authorship should be considered. If the previous work of prior individuals passes the Threshold of Effort test for any reason, the burden is on the PI involved to offer the opportunity for further engagement in the new manuscript to new individuals, preferably early in the process. If prior individuals respond positively and stay involved, then they should be authors on the new work. Prior individuals are responsible for deciding to stay engaged and following through with their involvement. Importantly, prior individuals should understand that if they do not respond to inquiries about authorship, historical datasets can still be used, but they will not be included as authors. This same approach may be followed for the advisors of prior individuals if they pass the Threshold."
  },
  {
    "objectID": "Authorship.html#author-order-and-etiquette",
    "href": "Authorship.html#author-order-and-etiquette",
    "title": "Authorship Agreement",
    "section": "",
    "text": "The order of authors on publications will follow the practice of first-last author emphasis. The first author will be the person who did the majority of the work, carried out the study, and will often be a the primary writer of the manuscript. Typically, the last author should be the lab PI. “Credit” or “importance” is attributed to authors in the following order: first, last, 2nd, 3rd, 4th, etc. Final decisions regarding author order ultimately lie with the first author, in consultation with their advisor, PI or mentor. Authors that disagree with the draft author order, however, should feel comfortable voicing their concerns. More importantly, primary authors should ask specifically for co-authors to approve the final order via email.\nAnyone listed as an author must be given a fair opportunity to read and comment on the manuscript. If prospective authors do not respond in a reasonable amount of time, they should be removed, barring exceptional circumstances (the response could be as simple as “manuscript is good to go”, as long as the author acknowledges and approves the content). The lead author should give AT LEAST two weeks for the response period and should specify a date by which comments are due. Two corollaries of this guideline are 1) anyone has the right to request removal as author from a paper for any reason, including a personal judgment of failure to cross the Threshold for Effort, and 2) no one should ever be an author on a manuscript where they did not approve the final submitted draft (note that many journals have this requirement).\nIt is the primary author’s responsibility, as corresponding author, to provide all co-authors with:\n\nA digital version of the final submitted draft\nNews of all significant correspondence with the editor/publisher\nThe opportunity to assist with revisions\nA digital version of all revisions submitted for publication and the responses to reviewers\nPage proofs and the opportunity to comment on them\nA final pdf of all published papers"
  },
  {
    "objectID": "Authorship.html#a-living-document",
    "href": "Authorship.html#a-living-document",
    "title": "Authorship Agreement",
    "section": "",
    "text": "Expectations for authorship are a set of evolving cultural norms. This means that 1) they must be taught anew to each set of students and/or new individual early in their involvement with the project and 2) the guidelines in this document need to be revisited and updated regularly (~annually or as needed)."
  },
  {
    "objectID": "Authorship.html#duty-as-a-co-author",
    "href": "Authorship.html#duty-as-a-co-author",
    "title": "Authorship Agreement",
    "section": "",
    "text": "The dissemination of research to the broader scientific community is not only part of the research process, but perhaps the ultimate reason for conducting research in the first place. There is an expectation and responsibility to share one’s work with the broader community (Cooke et al. 2014). It goes without saying that composing a manuscript by committee can be difficult. Co-authorship comes about in a variety of ways some of which may not require a formal role in writing but rather the collection of data, editing of the manuscript, etc. Some manuscripts are planned in advanced and have the luxury of identfying the core author pool. Other manuscripts are produced ad-hoc. Regardless of the origin it is recommended that if the manuscript has more than two authors it is advisable that a lead author along with a supporting person (typically senior author assumes this role) be identified. The objective of having two (or more) authors is to have these individuals generate a paper that is as close to final as possible, after all two-eyes are better than one. Letting other co-authors know that the paper is ready to move forward in the eyes of both the lead senior authors will often help to elicit a rapid review and response by co-authors.\nEveryone is busy, therefore some degree of time-management is needed when composing an manuscript. Therefore it is recommended that tentative deadlines/time-periods are discussed early and often. If necessary these deadline can be revised as needed due to changing priorities within reason but make sure these changes are communicated amongst all authors. Once a draft manuscript is produced and ready for co-author input provide a reasonable review deadline (couple of weeks). Typically, if co-authors are unresponsive its not they don’t find the manuscript important. It is possible that it has been shuffled down the dreaded “to do” list or they just forgot. This has happened to all of us at some point. Therefore a simple reminder would be helpful to re-engage the co-authors. Below is a flow chart from Cooke et al. (2014) with a suggested example of sequence of events to work with tardy or non-responsive co-authors.\n\n\n\n\n\nFlow-diagram with a possile sequence of events for working with tardy or non-responsive co-authors when preparing a manuscript for submission to a journal. From Cooke et al. (2014).\n\n\n\n\nAs suggested above, if co-authors are completely unresponsive it is the lead authors progrative, with input from co-authors to remove unresponsive co-authors. The Committee on Publication Ethics (COPE), has additional guidance information regarding authorship and ethics."
  },
  {
    "objectID": "Authorship.html#diversity-of-contributions",
    "href": "Authorship.html#diversity-of-contributions",
    "title": "Authorship Agreement",
    "section": "",
    "text": "Journals are increasingly requiring authors to specify roles according to author contributions. The days of honorary authorship is slowly (too slowly) coming to an end. However, there are many other aspects of research that are not adequately captured by the generally recognized short list of contributing roles, particularly in applied environmental disciplines such as conservation science, environmental science and applied ecology (herein referred to as conservation science). Cooke et al. (2021) provides some additional context and consideration to authorship guidelines reviewing existing and proposing potential expanded contributors roles.\nBottom line\nPoints summarized from the abstract of Cook et al. (2021).\n\nAuthorship should acknowledge and reward those deserving of credit. Being an author on a paper also means that one assumes some degree of ownership of the content, regardless of the authorship order. (Something, it feels, has been forgotten recently)\nThe growing recognition that authorship should reflect contributions that extend beyond the usual data collection, analysis and writing provides the ideal backdrop for rethinking contributions in conservation science. Cook et al. (2021) propose a more inclusive approach to authorship that recognizes and values diverse contributions and contributors using an expanded list of defined roles."
  },
  {
    "objectID": "Authorship.html#reference",
    "href": "Authorship.html#reference",
    "title": "Authorship Agreement",
    "section": "",
    "text": "Cooke SJ, Donaldson MR, Clark TD (2014) Practical guidance for early career researchers dealing with tardy or unresponsive co-authors. Ideas in Ecology and Evolution. 1(7). Link.\nCooke SJ, Nguyen VM, Young N, et al (2021) Contemporary authorship guidelines fail to recognize diverse contributions in conservation science research. Ecological Solutions and Evidence 2:e12060. Link.\nSaltmarsh Habitat & Avian Research Program (2014) Guidelines for authorship. Saltmarsh Habitat & Avian Research Program. https://www.tidalmarshbirds.org/."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Time series Analysis (Part II), How Much Data?\n\n\n\n\n\n\n\nblog\n\n\ntime series\n\n\npower\n\n\n\n\n\n\n\n\n\n\n\nNov 3, 2024\n\n\nPaul Julian\n\n\n\n\n\n\n  \n\n\n\n\nTime series Analysis (Part I), the basics\n\n\n\n\n\n\n\nblog\n\n\ntime series\n\n\n\n\n\n\n\n\n\n\n\nOct 26, 2024\n\n\nPaul Julian\n\n\n\n\n\n\n  \n\n\n\n\nPrevious Blog Posts and Migration\n\n\n\n\n\n\n\nblog\n\n\nmigration\n\n\n\n\n\n\n\n\n\n\n\nDec 16, 2023\n\n\nPaul Julian\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "news/2023-12-16-previous-posts.html",
    "href": "news/2023-12-16-previous-posts.html",
    "title": "Previous Blog Posts and Migration",
    "section": "",
    "text": "As I migrate posts from 2021-2022, please find them here on the prior blog page\nhttps://swampthingecology.org/blog/"
  },
  {
    "objectID": "news/2023-12-16-previous-posts.html#migrating-previous-posts",
    "href": "news/2023-12-16-previous-posts.html#migrating-previous-posts",
    "title": "Previous Blog Posts and Migration",
    "section": "",
    "text": "As I migrate posts from 2021-2022, please find them here on the prior blog page\nhttps://swampthingecology.org/blog/"
  },
  {
    "objectID": "news/20241103_Timeseries2.html",
    "href": "news/20241103_Timeseries2.html",
    "title": "Time series Analysis (Part II), How Much Data?",
    "section": "",
    "text": "In my last post I gave a relatively high level overview of time series and some basic analyses. Next we are going to talk about how much data is needed to perform some of these basic tests. Not necessarily the ins and outs (i.e. assumptions) of the statistical test(s) but how much data is needed to (1) run the test and (2) get a meaningful result. Most of this will be a discussion (with exercises) on statistical power. We will be also bring back our synthetic data functions introduced in the last post to provide some context. Most of the discussion will be centered around the Kendall family of trend tests."
  },
  {
    "objectID": "news/20241103_Timeseries2.html#minimum-number-of-samples",
    "href": "news/20241103_Timeseries2.html#minimum-number-of-samples",
    "title": "Time series Analysis (Part II), How Much Data?",
    "section": "Minimum number of samples",
    "text": "Minimum number of samples\nI often get asked how many samples do I need or what criterion did you use to screen the dataset to perform the trend test. Technically you only need two points to draw a line, but with those two points you don’t have the ability to understand the uncertainty (i.e. variance) of said line.\nWhats the absolute minimum needed to run the test? Most trend tests require 2 - 3 values to just do the math. Lets take the Kendall correlation as an example\n\n\n\nset.seed(123)\nyval2 &lt;- rnorm(2)*0.5\nxval2 &lt;- 1:length(yval2)\ncor.test(yval2,xval2,method=\"kendall\")\n\n\n    Kendall's rank correlation tau\n\ndata:  yval2 and xval2\nT = 1, p-value = 1\nalternative hypothesis: true tau is not equal to 0\nsample estimates:\ntau \n  1 \n\n\n\nset.seed(123)\nyval3 &lt;- rnorm(3)*0.5\nxval3 &lt;- 1:length(yval3)\ncor.test(yval3,xval3,method=\"kendall\")\n\n\n    Kendall's rank correlation tau\n\ndata:  yval3 and xval3\nT = 3, p-value = 0.3333\nalternative hypothesis: true tau is not equal to 0\nsample estimates:\ntau \n  1 \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat does this mean? For the Mann-Kendall test, the test statistic \\(S\\) is calculated as the sum of signs of differences between all pairs of observations therefore you only technically need two points to perform the test … but again this lacks any certainty and does not have a robust amount of data to accurately estimate the statistical likelihood. We can do a thought experiment to look at the number of samples to calculate the exact critical values for the \\(S\\) statistic. This can be achieved by calculating all possible \\(S\\) values for a given sample size \\(n\\) and determining the probabilities of obtaining these values under the null hypothesis of no trend.\nHere is the basic functions needed to generate the critical value look-up table for the Mann-Kendall \\(S\\) statistic.\n\ncalculate_S: This function calculates the Mann-Kendall \\(S\\)-Statistic for a given vector (x). It loops over all pairs \\((i,j)\\) to compute the sum of signs. In the correlation test examples above this is the T statistic.\ncalculate_Smax: This function calculates the \\(S_{max}\\) value for a given sample size (\\(n\\)).\ngenerate_lookup_table: This function generates all possible permutations of ranks for a given sample size \\(n\\), calculates \\(S\\) for each permutation, and then tabulates the frequency of each unique \\(S\\) value. It calculates the probability of each \\(S\\)-value by dividing its frequency by the total number of permutations.\n\n\nlibrary(gtools) # for permutations\n\n# Define function to calculate Mann-Kendall S statistic\ncalculate_S &lt;- function(x) {\n  n &lt;- length(x)\n  S &lt;- 0\n  for (i in 1:(n - 1)) {\n    for (j in (i + 1):n) {\n      S &lt;- S + sign(x[j] - x[i])\n    }\n  }\n  return(S)\n}\n\ncalculate_Smax &lt;- function(n){n*(n-1)/2}\n\n# Function to generate look-up table for a given sample size n\ngenerate_lookup_table &lt;- function(n) {\n  # Generate all possible permutations of ranks\n  rank_permutations &lt;- permutations(n, n, v = 1:n)\n  \n  # Calculate S for each permutation\n  S_values &lt;- apply(rank_permutations, 1, calculate_S)\n  \n  # Count frequency of each S value\n  S_table &lt;- table(S_values)\n  \n  # Calculate probabilities\n  total_permutations &lt;- factorial(n)\n  prob_table &lt;- data.frame(\n    S = as.numeric(names(S_table)),\n    Frequency = as.integer(S_table),\n    # Smax = as.integer(n*(n-1)/2),\n    Probability = as.integer(S_table) / total_permutations\n  )\n  \n  return(prob_table)\n}\n\nLets look at samples sizes of 2, 3, 4 and 5. Any more than that and the process get bogged down (due to the computation of individual permutations). Lets use the lapply function to generate look up tables for a range of n values.\n\n# Generate look-up tables for sample sizes n = 2, 3, 4, 5 and 10\nlookup_tables &lt;- lapply(2:5, generate_lookup_table)\nsmax_val &lt;- calculate_Smax(2:5)\nnames(lookup_tables) &lt;- paste0(\"n=\", 2:5,\"; Smax = \",smax_val)\n\n# Display the look-up tables\nlookup_tables\n\n$`n=2; Smax = 1`\n   S Frequency Probability\n1 -1         1         0.5\n2  1         1         0.5\n\n$`n=3; Smax = 3`\n   S Frequency Probability\n1 -3         1   0.1666667\n2 -1         2   0.3333333\n3  1         2   0.3333333\n4  3         1   0.1666667\n\n$`n=4; Smax = 6`\n   S Frequency Probability\n1 -6         1  0.04166667\n2 -4         3  0.12500000\n3 -2         5  0.20833333\n4  0         6  0.25000000\n5  2         5  0.20833333\n6  4         3  0.12500000\n7  6         1  0.04166667\n\n$`n=5; Smax = 10`\n     S Frequency Probability\n1  -10         1 0.008333333\n2   -8         4 0.033333333\n3   -6         9 0.075000000\n4   -4        15 0.125000000\n5   -2        20 0.166666667\n6    0        22 0.183333333\n7    2        20 0.166666667\n8    4        15 0.125000000\n9    6         9 0.075000000\n10   8         4 0.033333333\n11  10         1 0.008333333\n\n\nEach look-up table contains:\n\nS: The possible Mann-Kendall statics values\nFrequency: The frequency of each \\(S\\)-value among all permutations\nProbability: The probability of obtaining each S-value under the null hypothesis.\nSmax: The maximum possible value of \\(S\\) for a given sample size. \\(S_{max}\\) is useful for determining critical values and conducting hypothesis tests in the Mann-Kendall trend test, particularly when assessing the significance of the observed \\(S\\)-statistic under the null hypothesis of no trend.\n\nAs you can see a sample size of two (n=2) you have a an equal chance (probability) of accepting or rejecting your hypothesis.\nWhile this section pertains to how few samples are needed to do the test, its worth mentioning the other side of the coin. When sample sizes increase some of the mathematics changes slightly. For instance when \\(n\\) is large (a relative term … lets just say \\(n\\ge10\\)), the standardized \\(S\\) (often denoted as \\(Z\\)) approximately follows a normal distribution. This allows the use of z-scores to determine statistical significance of the observed \\(S\\)-statistic.\nNow that we know the fundamentals of two points make a straight line … we can move onto statistical power."
  },
  {
    "objectID": "news/20241103_Timeseries2.html#how-to-do-a-power-analysis",
    "href": "news/20241103_Timeseries2.html#how-to-do-a-power-analysis",
    "title": "Time series Analysis (Part II), How Much Data?",
    "section": "How to do a power analysis",
    "text": "How to do a power analysis\nStatistical power analysis can be performed a couple different ways. For this post, lets assume this is a post-hoc statistical power analysis (remember you can do as power analysis before collecting samples to determine how many samples you need). In this example we will use a power calculation for a general linear model to approximate the power. The reason for the caveat is a general linear model is NOT a trend test (unless the data and model residuals fit the assumptions of the test … fat chance) and is different from the the Mann-Kendall and Thiel-Sen estimator.\n\npwr\nThe pwr::pwr.f2.test(...) is a function to calculate the statistical power for a linear model. Its pretty straight forward …\n\nlibrary(pwr)\n\n# Define parameters\neffect_size &lt;- 0.15  # Estimate of the effect size (can be calculated or approximated)\nalpha &lt;- 0.05        # Significance level\npower &lt;- 0.8         # Desired power\n\n# Perform power analysis for linear regression\nsample_size &lt;- pwr.f2.test(u = 1, f2 = effect_size, sig.level = alpha, power = power)\n\n# Output the required number of samples\nsample_size\n\n\n     Multiple regression power calculation \n\n              u = 1\n              v = 52.315\n             f2 = 0.15\n      sig.level = 0.05\n          power = 0.8\n\n\n\nu = 1 represents one predictor variable (time, in this case).\nf2 is the effect size (Cohen’s \\(f^{2}\\)) for linear models and is calculated using the models \\(R_{2}\\).\n\n\n\\(f^{2} = \\frac{R^{2}}{1 - R^{2}}\\)\n\nOf course knowing the effect size and power is half the information we want to know. Another way to do a power analysis and probably a more meaningful way would be a simulation based test. I’ll show you two different versions, a very basic version and a more elaborate version based off an analysis presented by Dr Schramm  in this paper.\n\n\nBasic Simulation\n\nset.seed(123)\n# Parameters\ntrue_slope &lt;- 0.05  # Slope of the trend\nsigma &lt;- 1          # Standard deviation of errors\nn_sim &lt;- 1000       # Number of simulations\nsample_sizes &lt;- seq(20, 100, by = 5)  # Range of sample sizes to test\nalpha &lt;- 0.05\n\n# Function to simulate data and fit regression model\npower_simulation &lt;- function(n) {\n  significant_results &lt;- 0\n  for (i in 1:n_sim) {\n    x &lt;- 1:n\n    y &lt;- true_slope * x + rnorm(n, mean = 0, sd = sigma)\n    model &lt;- lm(y ~ x)\n    if (summary(model)$coefficients[2, 4] &lt; alpha) {  # p-value of slope\n      significant_results &lt;- significant_results + 1\n    }\n  }\n  return(significant_results / n_sim)\n}\n\n# Run simulations for each sample size\npower_results &lt;- sapply(sample_sizes, power_simulation)\n\n\n\n\n\n\nStatistical power by sample size.\n\n\n\n\nA power of 0.80 is typically considered appropriate, which equates to a 20% chance of encountering a Type II error. Remember from above the definition of power, if we are sampling from a population where the null hypothesis is false, the power is calculated as\n\n\\[ Power = \\frac{N_{rejected}{N}\\]\n\nwhere \\(N\\) is the total number of tests and \\(N_{rejected}\\) are the total number of times the test rejected the null hypothesis. I highly recommend reading this paper for a great IRL example of estimating statistical power in trend analyses. I’ve personally used variants of this but they haven’t officially made it into my peer-reviewed pubs yet.\n\n\nSimulation Based Power Analysis\nSimilar to our our dat2 time series from the prior post we are adding in some extra noise.\n\nn.yrs &lt;- 20\nn.vals &lt;- n.yrs*12 \n\ndat &lt;- simulate_timeseries_vol(\n  n = n.vals,           # About 20 years of daily data\n  trend_slope = 0.05,   # Upward trend\n  seasonal_amp = 2,     # Base seasonal amplitude\n  seasonal_period = 12, # Monthly seasonality\n  init_vol = 0.5,      # Initial volatility\n  vol_persistence = 0.65,\n  rw_sd = 0.3\n)\n## add some years and months\ndat$timeseries  &lt;-  cbind(dat$timeseries,\n                           expand.grid(Mon = 1:12, Yr = 1990:(1990+(n.yrs-1)))\n)\ndat$timeseries$date &lt;-  with(dat$timeseries,as.Date(paste(Yr,Mon,\"01\",sep=\"-\")))\n\n\n\n\n\n\nThe simulated time series.\n\n\n\n\nLets aggregate the data to year (Yr) for this example by calculating the annual mean\n\nlibrary(plyr)\ndat_yr &lt;- ddply(dat$timeseries,c(\"Yr\"),summarise,\n                mean.val = mean(value),\n                sd.val = sd(value)\n                )\n\n\n\n\n\n\nQuick plots of annually aggregated data.\n\n\n\n\nMuch like the basic simulation power analysis presented about this method will also use a simulation.\n\nlibrary(mblm)\n\nyrs &lt;- seq(min(dat_yr$Yr),max(dat_yr$Yr))\nyrs2 &lt;- yrs[3:length(yrs)]\n\nnsims &lt;- 1000\npvalues &lt;- NA\npower.trend.detect &lt;- data.frame()\nset.seed(123)\nfor(i in 1:length(yrs2)){\n  \n  tmp.dat &lt;- subset(dat_yr,Yr%in%seq(yrs[1],yrs2[i]))\n  \n  for (j in 1:nsims) {\n    yval &lt;- rnorm(n=length(tmp.dat$mean.val),\n                  mean=tmp.dat$mean.val,\n                  sd=sd(tmp.dat$mean.val))\n    trend.test &lt;- with(tmp.dat,\n                       cor.test(yval,Yr,method=\"kendall\"))\n    pvalues[j] &lt;-  trend.test$p.value\n  }\n  \n  thiel_sen  &lt;-  mblm(mean.val~Yr,tmp.dat)\n  trend &lt;- with(tmp.dat,cor.test(mean.val,Yr,method=\"kendall\"))\n  \n  power  &lt;-  sum(pvalues &lt; 0.05)/nsims\n  rslt  &lt;- data.frame(Yr=yrs2[i],\n                      slope = as.numeric(coefficients(thiel_sen)[2]),\n                      kendall.tau = as.numeric(trend$estimate),\n                      kendall.pval = as.numeric(trend$p.value),\n                      trend.power=power)\n  power.trend.detect  &lt;- rbind(power.trend.detect,rslt)\n}\npower.trend.detect$yrs &lt;- power.trend.detect$Yr-min(dat_yr$Yr)\n\npower.trend.detect\n\n     Yr     slope kendall.tau kendall.pval trend.power yrs\n1  1992 0.3503085   0.3333333 1.000000e+00       0.000   2\n2  1993 0.5555924   0.6666667 3.333333e-01       0.000   3\n3  1994 0.7112633   0.8000000 8.333333e-02       0.096   4\n4  1995 0.7071783   0.8666667 1.666667e-02       0.161   5\n5  1996 0.6867529   0.9047619 2.777778e-03       0.290   6\n6  1997 0.7071783   0.9285714 3.968254e-04       0.411   7\n7  1998 0.7596028   0.9444444 4.960317e-05       0.549   8\n8  1999 0.7071783   0.9111111 2.976190e-05       0.620   9\n9  2000 0.6916715   0.8545455 4.624619e-05       0.643  10\n10 2001 0.6722881   0.8484848 1.634233e-05       0.742  11\n11 2002 0.6712462   0.7179487 2.839785e-04       0.616  12\n12 2003 0.6504437   0.6483516 7.575015e-04       0.620  13\n13 2004 0.5643232   0.6571429 3.303709e-04       0.688  14\n14 2005 0.5611362   0.6833333 8.266192e-05       0.721  15\n15 2006 0.4400709   0.7205882 1.148789e-05       0.750  16\n16 2007 0.4299183   0.7516340 1.442393e-06       0.856  17\n17 2008 0.4121871   0.7777778 1.649783e-07       0.878  18\n18 2009 0.4167471   0.8000000 1.731000e-08       0.926  19\n\n\n\n\n\n\n\nSimulation power analysis of annual trend on the dat dataset\n\n\n\n\nBased on this analysis, due to the variability in the data it took almost 18 simulated years of data before it reached a power of ≥ 0.80. Meanwhile, we didn’t see a “significant” trend until year five despite a relative low “power”. As more years were added, the power increased. Now this is based on annually aggregated data and an annual trend test. This could also be done on a seasonal Kendall trend test, which probably show different results given the seasonal nature of the data. We can also tune the knobs on the synthetic data function to see how things change given all the factors.\nGiven all this trend analyses and power of the test are dependent upon several factors, including getting at the minimum number of samples needed. These factors include, but are not limited to:\n\nTrend Strength: For a “strong” trend, fewer points might suffice, but “weaker” trends require more data points to confidently discern a pattern.\nData Variability: Higher variability in the data means more points are needed to distinguish a trend from random fluctuations.\nSeasonality and Autocorrelation: If there’s seasonality or serial correlation, you may need to account for it or have more data points to ensure the test’s assumptions are met."
  }
]