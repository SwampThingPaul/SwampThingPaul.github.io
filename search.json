[
  {
    "objectID": "Publications.html",
    "href": "Publications.html",
    "title": "Publications",
    "section": "",
    "text": "Information extracted from Google Scholar Profile\nGoogle Scholar Citations = 359  H-Index = 11  i10-Index = 13 \nInformation and definitions of various indices"
  },
  {
    "objectID": "Publications.html#publication-metrics",
    "href": "Publications.html#publication-metrics",
    "title": "Publications",
    "section": "",
    "text": "Information extracted from Google Scholar Profile\nGoogle Scholar Citations = 359  H-Index = 11  i10-Index = 13 \nInformation and definitions of various indices"
  },
  {
    "objectID": "Publications.html#books",
    "href": "Publications.html#books",
    "title": "Publications",
    "section": "Books",
    "text": "Books"
  },
  {
    "objectID": "Publications.html#peer-review-literature",
    "href": "Publications.html#peer-review-literature",
    "title": "Publications",
    "section": "Peer Review Literature",
    "text": "Peer Review Literature\nCarey, J., K. Jankowski, P. Julian, L. Sethna, P. Thomas, and J. J. Rohweder. 2019. Exploring Silica Stoichiometry on a Large Floodplain Riverscape. Frontiers in Ecology and Evolution 7: 346. Husk, B., P. Julian, N. Tromas, D. Phan, K. Painter, H. M. Baulch, and S. Sauvé. 2023. Improving Water Quality in a Hypereutrophic Lake and Tributary Through Agricultural Nutrient Mitigation: A Multi-Year Monitoring Analysis.doi:10.2139/ssrn.4572355 Jankowski, K. J., K. Johnson, L. Sethna, and others. 2023. Long-Term Changes in Concentration and Yield of Riverine Dissolved Silicon From the Poles to the Tropics. Global Biogeochemical Cycles 37: e2022GB007678. doi:10.1029/2022GB007678 Julian, P. 2011. Home range dynamics of female Florida panthers in reponse to kitten production. Florida Scientist 74. Julian, P. 2013a. Comment on Spatial and temporal phosphorus distribution changes in a large wetland ecosystem by X. Zapata-Rios et al.: Commentary. Water Resources Research 49: 2312–2313. doi:10.1002/wrcr.20162 Julian, P. 2013b. Mercury Bio-concentration Factor in Mosquito Fish (Gambusia spp.) in the Florida Everglades. Bulletin of Environmental Contamination and Toxicology 90: 329–332. doi:10.1007/s00128-012-0939-6 Julian, P. 2013c. Mercury hotspot identification in Water Conservation Area 3, Florida, USA. Annals of GIS 19: 79–88. doi:10.1080/19475683.2013.782469 Julian, P. 2014. Reply to Mercury Bioaccumulation and Bioaccumulation Factors for Everglades Mosquitofish as Related to Sulfate: A Re-Analysis of Julian II (2013). Bulletin of Environmental Contamination and Toxicology 93: 517–521. doi:10.1007/s00128-014-1389-0 Julian, P. 2015. South Florida Coastal Sediment Ecological Risk Assessment. Bulletin of environmental contamination and toxicology 95: 188–193. Julian, P. 2016. Commentary on Mitsch et al., 2015, Protecting the Florida Everglades wetlands with wetlands: Can stormwater phosphorus be reduced to oligotrophic conditions? Ecological Engineering 108: 333–337. Julian, P. 2017a. Assessment of Upper Taylor Slough water quality and implications for ecosystem management in Everglades National Park. Wetlands Ecology and Management 25: 191–209. doi:10.1007/s11273-016-9509-8 Julian, P. 2017b. Letter to the Editor regarding Surratt D, Shindle D, Yongshan W, et al. Letter to the Editor regarding: Julian P, 2017. Assessment of Upper Taylor Slough water quality and implications for ecosystem management in Everglades National Park. Wetlands Ecol Manage 1–3. doi:10.1007/s11273-017-9571-x Julian, P. 2019. Spatial Ecology and Conservation Modeling. Applications with R. Robert Fletcher, Marie-Josee Fortin: Book Review. Austral Ecology. doi:10.1111/aec.12791 Julian, P. 2020. Getting the science right to protect and restore our environment. A critique of Lapointe et al. (2019) Nitrogen enrichment, altered stoichiometry, and coral reef decline at Looe Key, Florida Keys, USA: a 3-decade study. Mar Biol 167: 68. doi:10.1007/s00227-020-3667-1 Julian, P., R. Chambers, and T. Russell. 2017a. Iron and Pyritization in Wetland Soils of the Florida Coastal Everglades. Estuaries and Coasts 40: 822–831. doi:10.1007/s12237-016-0180-3 Julian, P., and M. W. Cunningham. 2013. Total mercury concentration in Florida black bear (Ursus americanus floridanus). Florida Scientist 76. Julian, P., E. M. Everham III, and M. B. Main. 2012. Influence of a Large-scale Removal of an Invasive Plant (Melaleuca quinquenervia) on Home-range Size and Habitat Selection by Female Florida Panthers (Puma concolor coryi) within Big Cypress National Preserve, Florida. Southeastern Naturalist 11: 337–348. Julian, P., J. Fourqurean, S. Davis, and others. Submitted. Long-term spatiotemporal patterns and trends in water quality reveal a coastal continuum of disturbance legacies. Limnology and Oceanography. doi:https://doi.org/10.21203/rs.3.rs-1753636/v1 Julian, P., S. Gerber, R. K. Bhomia, J. King, T. Z. Osborne, and A. L. Wright. 2020. Understanding stoichiometric mechanisms of nutrient retention in wetland macrophytes: stoichiometric homeostasis along a nutrient gradient in a subtropical wetland. Oecologia. doi:10.1007/s00442-020-04722-9 Julian, P., S. Gerber, R. K. Bhomia, J. King, T. Z. Osborne, A. L. Wright, M. Powers, and J. Dombrowski. 2019. Evaluation of nutrient stoichiometric relationships among ecosystem compartments of a subtropical treatment wetland. Do we have “Redfield wetlands”? Ecol Process 8: 20. doi:10.1186/s13717-019-0172-x Julian, P., S. Gerber, A. L. Wright, B. Gu, and T. Z. Osborne. 2017b. Carbon pool trends and dynamics within a subtropical peatland during long-term restoration. Ecol Process 6: 43–57. doi:10.1186/s13717-017-0110-8 Julian, P., and B. Gu. 2015. Mercury accumulation in largemouth bass (Micropterus salmoides Lacépède) within marsh ecosystems of the Florida Everglades, USA. Ecotoxicology 24: 202–214. doi:10.1007/s10646-014-1373-9 Julian, P., B. Gu, and G. Redfield. 2015. Comment on and Reinterpretation of Gabriel et al. (2014) Fish Mercury and Surface Water Sulfate Relationships in the Everglades Protection Area. Environmental Management 55: 1–5. doi:10.1007/s00267-014-0377-9 Julian, P., B. Gu, and A. L. Wright. 2016a. Mercury Stoichiometric Relationships in a Subtropical Peatland. Water Air Soil Pollut 227: 472. doi:10.1007/s11270-016-3180-9 Julian, P., and T. Z. Osborne. 2018. From lake to estuary, the tale of two waters: A study of aquatic continuum biogeochemistry. Environment Monitoring and Assessment 190: 1–24. doi:https://doi.org/10.1007/s10661-017-6455-8 Julian, P., T. Z. Osborne, R. K. Bhomia, and O. Villapando. 2021. Knowing your limits: evaluating aquatic metabolism in a subtropical treatment wetland. Hydrobiologia. doi:10.1007/s10750-021-04617-7 Julian, P., T. Z. Osborne, and R. Ellis. 2023a. Evaluation of Biogeochemical Changes in Channelized and Restored Portions of a Subtropical Floodplain. Hydrobiology 2: 1–18. doi:10.3390/hydrobiology2010001 Julian, P., and L. Reidenbach. Submitted. Upstream water management and its role in estuary health, evaluation of freshwater management and subtropical estuary function. Water Resources Management. Julian, P., T. Schafer, M. J. Cohen, P. Jones, and T. Z. Osborne. 2023b. Changes in the spatial distribution of total phosphorus in sediment and water column of a shallow subtropical lake. Lake and Reservoir Management 39: 213–230. Julian, P., M. Thompson, and E. C. Milbrandt. 2024. Dark waters: Evaluating seagrass community response to optical water quality and freshwater discharges in a highly managed subtropical estuary. Regional Studies in Marine Science 69: 103302. doi:10.1016/j.rsma.2023.103302 Julian, P., and Z. Welch. 2022. Understanding the ups and downs, application of hydrologic restoration measures for a large Subtropical Lake. Lake and Reservoir Management 38: 304–317. doi:https://doi.org/10.21203/rs.3.rs-1739423/v2 Julian, P., A. L. Wright, and T. Z. Osborne. 2016b. Iron and sulfur porewater and surface water biogeochemical interactions in subtropical peatlands. Soil Science Society of America Journal 80: 794–802. Kominoski, J. S., E. E. Gaiser, E. Castaneda‐Moya, and others. 2020. Disturbance legacies increase and synchronize nutrient concentrations and bacterial productivity in coastal ecosystems. Ecology 101. doi:https://doi.org/10.1002/ecy.2988 Marazzi, L., C. M. Finlayson, P. A. Gell, P. Julian, J. S. Kominoski, and E. E. Gaiser. 2018. Balancing wetland restoration benefits to people and nature. Solutions Journal 9. Schafer, T. B., P. Julian, O. Villapando, and T. Z. Osborne. 2023. Abiotic mineralization of dissolved organic phosphorus for improved nutrient retention in a large-scale treatment wetland system. Ecological Engineering 195: 107078. doi:10.1016/j.ecoleng.2023.107078 Schafer, T., N. Ward, P. Julian, K. R. Reddy, and T. Z. Osborne. 2020. Impacts of Hurricane Disturbance on Water Quality across the Aquatic Continuum of a Blackwater River to Estuary Complex. Journal of Marine Science and Engineering 8: 412. doi:10.3390/jmse8060412 Smith, M. C., P. Julian, D. DeAngelis, and B. Zhang. 2023. Ecological benefits of integrative weed management of Melaleuca quinquenervia in Big Cypress National Preserve. BioControl. doi:10.1007/s10526-023-10229-y"
  },
  {
    "objectID": "Publications.html#technical-literature",
    "href": "Publications.html#technical-literature",
    "title": "Publications",
    "section": "Technical Literature",
    "text": "Technical Literature"
  },
  {
    "objectID": "news/20241117_Timeseries3.html",
    "href": "news/20241117_Timeseries3.html",
    "title": "Time series Analysis (Part III), Welcome to the Kendall Family?",
    "section": "",
    "text": "So far we’ve gone over the basics of time series (Part I), minimum number of samples, power analyses and some basic trend analyses (Part II). In this post we will talk more explicitly about the Kendall family of trend test, more specifically seasonal trends."
  },
  {
    "objectID": "news/20241117_Timeseries3.html#kendall-trend-test",
    "href": "news/20241117_Timeseries3.html#kendall-trend-test",
    "title": "Time series Analysis (Part III), Welcome to the Kendall Family?",
    "section": "Kendall Trend Test",
    "text": "Kendall Trend Test\nWhile this might come down to semantics but across the literature you’ll seen Mann-Kendall (trend) test and Kendall trend test, test or rank correlation test. Overall these tests are one in the same, its partly in how they are applied (to some degree). For instance the Mann-Kendall test specifically used to detect monotonic trend in a time series while Kendall tests whether two variables are related in a monotonic way, irrespective of temporal structure. Moreover, the Mann-Kendall test incorporates the temporal order of observation to assess whether values are consistently increasing or decreasing over time. Therefore, one could say (and the EnvStats help page for kendallTrendTest provides a detailed discussion) that the Mann-Kendall is a special case of the test for independence based on the Kendall’s tau statistic. Which both the &#120591 (Kendall) and S-statistic (Mann-Kendall) both measure the ratio of concordant and discordant pairs, its just the Mann-Kendall interpretation includes a time order. If you dig into the source code for stats::cor.test() and EnvStats::kendallTrendTest you’ll see things are calculated nearly identically.\nWhile the Kendall (or Mann-Kendall in our case) is a non-parametric statistic, as in there are no assumptions made about the distribution of the X (time) and Y variables there are some assumptions of the test.\n\nMonotonic Trend: the test assumes (hence its use to test the hypothesis) the presence of a monotonic trend in the data (irrespective of direction) but does not assume linearity.\n\nIf the data set has periodic components it could mislead results. Below is an example where one dataset is monotonically increasing (left) while the other is increasing there is a strong seasonal component (right), both are significantly increasing. If you do the trend analysis, this variability in the second (right) dataset is reflected in the lower (tau or S-statistic … depending on the test).\n\n\n\n\n\n\n\nContrasting simulated time series realtive to trend.\n\n\n\n\n\nData Independence: the observations in the time series are assumed to be independent. If there is serial autocorrelation then the test statistic could be biased. However, given the nature of the data, serial correlations are sometimes unavoidable and generally pose a challenge. However, there are ways to check and methods, such as “pre-whitening” to account for it.\n\nTo check for serial correlation a simple autocorrelation function can be used. There are other tests like the Durbin-Watson test lmtest::dwtest or the Breusch-Godfrey test lmtest::bgtest but these test evaluate the residuals of the model to evaluate autocorrelation not autocorrelation of the data itself like acf.\n\n\n\n\n\nACF plot of the simulated time series datasets.\n\n\n\n\nHere is a pre-whitening method that can be implemented without any additional packages. The pre-whitening procedure accounts for autocorrelation and removes its “effect” by performing the trend analysis on the residuals of the ar model. For more reading on pre-whitening here is a great paper I’ve come across and frequently come back to.\nYue S, Pilon P. 2004. A comparison of the power of the t test, Mann-Kendall and bootstrap tests for trend detection. Hydrological Sciences Journal. 49:1–37. https://doi.org/10.1623/hysj.49.1.21.53996\n\n# simulated data (used simulate_timeseries function from prior posts)\ntest  &lt;-  simulate_timeseries(n=n.vals,\n                              seasonal_period = 1,\n                              seasonal_amp=1,\n                              trend_slope=0.1)\ntest$timeseries  &lt;-  cbind(test$timeseries,\n                           expand.grid(Mon = 1:12, Yr = 1990:(1990+(n.yrs-1)))\n)\ntest$timeseries$date &lt;-  with(test$timeseries,as.Date(paste(Yr,Mon,\"01\",sep=\"-\")))\ntest$timeseries$dec.date &lt;- lubridate::decimal_date(test$timeseries$date)\n\n\n# Fit AR(1) Model\nar_model &lt;- ar(test$timeseries$value, order.max = 1, \n               method = \"yule-walker\")\n# ar_model$order  # Optimal order selected\n# ar_model$ar     # Estimated AR(1) coefficient\ntest$timeseries$ar.residuals &lt;- ar_model$resid\n# plot(ar_model$resid, type = \"l\", main = \"Residuals\")\n\n# Perform Kendall Test on Residuals\nwith(test$timeseries,cor.test(ar.residuals,dec.date,method=\"kendall\"))\n\n\n    Kendall's rank correlation tau\n\ndata:  ar.residuals and dec.date\nz = 2.6877, p-value = 0.007194\nalternative hypothesis: true tau is not equal to 0\nsample estimates:\n      tau \n0.2402104 \n\nacf(test$timeseries$ar.residuals,na.action=na.pass,\n    col=\"dodgerblue1\",lwd=2,main=NA)\n\n\n\n\n\n\n\n# another way to do it (like zyp R-package)\ndata &lt;- test$timeseries$value\nc &lt;- acf(data,lag.max=1,plot=FALSE,na.action=na.pass)$acf[2]\nn &lt;- length(test$timeseries$value)\n\nresiduals2 &lt;- (data[2:n] - c * data[1:(n-1)]) / (1 - c)\npw.time &lt;- test$timeseries$dec.date[1:(n-1)]\n\ncor.test(residuals2,pw.time,method=\"kendall\")\n\n\n    Kendall's rank correlation tau\n\ndata:  residuals2 and pw.time\nz = 2.6877, p-value = 0.007194\nalternative hypothesis: true tau is not equal to 0\nsample estimates:\n      tau \n0.2402104 \n\nacf(residuals2,na.action=na.pass,col=\"dodgerblue1\",lwd=2,main=NA)\n\n\n\n\n\n\n\n\nYou get the same results!!\n\nHomogeneity: The data must come from a homogeneous population, meaning that the process generating the data does not change systematically over time.\nHandling Ties: The test assumes that ties (equal values in the data) are either minimal or appropriately accounted for.\nSufficient Sample Size: A sufficiently large sample size is required to achieve reliable results, particularly for datasets with variability. See last post about sample size and statistical power.\n\nWhile data can be aggregated to be tested using the Mann-Kendall test, if the data has a strong seasonal component or serial dependence then its usually recommended (depending on the question being asked) to use the seasonal Mann-Kendall Test to adjust for autocorrelation."
  },
  {
    "objectID": "news/20241117_Timeseries3.html#seasonal-mann-kendall-trend-test",
    "href": "news/20241117_Timeseries3.html#seasonal-mann-kendall-trend-test",
    "title": "Time series Analysis (Part III), Welcome to the Kendall Family?",
    "section": "Seasonal Mann-Kendall Trend Test",
    "text": "Seasonal Mann-Kendall Trend Test\nMost of the assumptions of the Seasonal Mann-Kendall trend test are similar to those of the Mann-Kendall test, just most are applied to the seasonal data. The key assumptions are:\n\nSeasonal Independence: Each group or season is treated as an independent data set and therefore the seasonal trends are independent of one another.\nConsistency in Seasonal Grouping: The data must be grouped into consistent and comparable seasons or time periods (e.g., months, quarters). The length of seasons should be uniform across the data set. Other methods could be applied if the start of a particular season is not consistent (stay tuned).\nHomogeneity of Data within Seasons: The data must come from a homogeneous population during a particular season. Some text regarding the method also references data coming from the same distribution within each season, however I have not been able to find a consistent method. The EnvStats package does have a heterogeneity test built into the kendallSeasonalTrendTest() but this test focus on evaluating if there is heterogeneity of trend direction across seasons.\nTrend within Seasons is Monotonic: similar to Mann-Kendall assumptions above but specific for each season.\n\nWhen doing the seasonal Mann-Kendall test, the one thing I check first is the Heterogeneity Test for Trend. As mentioned above, the EnvStats::kendallSeasonalTrendTest() performed this test along with all the other trend analyses. Here is an example.\n\ntest2 &lt;-  simulate_timeseries(n=n.vals,\n                              seasonal_period = 4,\n                              seasonal_amp=2,trend_slope=0.1)\ntest2$timeseries  &lt;-  cbind(test2$timeseries,\n                           expand.grid(Mon = 1:12, Yr = 1990:(1990+(n.yrs-1)))\n)\ntest2$timeseries$date &lt;-  with(test2$timeseries,as.Date(paste(Yr,Mon,\"01\",sep=\"-\")))\ntest2$timeseries$dec.date &lt;- lubridate::decimal_date(test2$timeseries$date)\n\nWe will assume that month is the season of interest. This could also be broken into meterological, or hydrological seasons as well, but they have to be numeric.\n\nlibrary(EnvStats)\nkendallSeasonalTrendTest(value~Mon+Yr,test2$timeseries)\n\n\nResults of Hypothesis Test\n--------------------------\n\nNull Hypothesis:                 All 12 values of tau = 0\n\nAlternative Hypothesis:          The seasonal taus are not all equal\n                                 (Chi-Square Heterogeneity Test)\n                                 At least one seasonal tau != 0\n                                 and all non-zero tau's have the\n                                 same sign (z Trend Test)\n\nTest Name:                       Seasonal Kendall Test for Trend\n                                 (with continuity correction)\n\nEstimated Parameter(s):          tau       =     0.9666667\n                                 slope     =     1.2048690\n                                 intercept = -2383.2278857\n\nEstimation Method:               tau:        Weighted Average of\n                                             Seasonal Estimates\n                                 slope:      Hirsch et al.'s\n                                             Modification of\n                                             Thiel/Sen Estimator\n                                 intercept:  Median of\n                                             Seasonal Estimates\n\nData:                            y      = value\n                                 season = Mon  \n                                 year   = Yr   \n\nData Source:                     test2$timeseries\n\nSample Sizes:                    1     =  5\n                                 2     =  5\n                                 3     =  5\n                                 4     =  5\n                                 5     =  5\n                                 6     =  5\n                                 7     =  5\n                                 8     =  5\n                                 9     =  5\n                                 10    =  5\n                                 11    =  5\n                                 12    =  5\n                                 Total = 60\n\nTest Statistics:                 Chi-Square (Het) = 0.400000\n                                 z (Trend)        = 8.131728\n\nTest Statistic Parameter:        df = 11\n\nP-values:                        Chi-Square (Het) = 9.999996e-01\n                                 z (Trend)        = 4.232137e-16\n\nConfidence Interval for:         slope\n\nConfidence Interval Method:      Gilbert's Modification of\n                                 Theil/Sen Method\n\nConfidence Interval Type:        two-sided\n\nConfidence Level:                95%\n\nConfidence Interval:             LCL = 1.110484\n                                 UCL = 1.333063\n\n\nIn the results you see Test Statistics: Chi-Square (Het) and P-values: Chi-Square (Het) these are the results of the Van Belle-Hughes Heterogeneity Test for Trend. If you wanted to do this outside of the EnvStats package you can with the following example.\n\nlibrary(reshape2)\n## Format the data\ntest2.df &lt;- dcast(test2$timeseries,Yr~Mon,value.var=\"value\",mean)\n\n#yr.vals &lt;- as.numeric(names(test2.df[, -1]))\nmk_results &lt;- apply(test2.df[, -1], 1,\n                     FUN = function(x){\n                       tmp &lt;- cor.test(x=x,y=1:length(x),method=\"kendall\")\n                       tmp2 &lt;- data.frame(tau=as.numeric(tmp$estimate),\n                                          pval=as.numeric(tmp$p.value)\n                       )\n                       return(tmp2)\n                      }\n)\n                        \n\n# Extract p-values and tau values\np_values &lt;- sapply(mk_results, function(x) x$pval)  # p-values\ntau_values &lt;- sapply(mk_results, function(x) x$tau)  # Tau values\n\n# Perform a chi-square test for heterogeneity\nchi_square_result &lt;- chisq.test(p_values)\nchi_square_result\n\n\n    Chi-squared test for given probabilities\n\ndata:  p_values\nX-squared = 0.23069, df = 4, p-value = 0.9938\n\n\nWhile the chi-squared statistic differs slightly, the p-value remains the same and does a good job approximating the method in EnvStats.\nThis can also be done using the Kendall R-package like this:\n\nlibrary(Kendall)\n\n## Format the data\ntest2.df &lt;- dcast(test2$timeseries,Mon~Yr,value.var=\"value\",mean)\n\n# Apply Mann-Kendall test for trend on each time series (column). \nmk_results &lt;- lapply(test2.df[, -1], MannKendall)\n\np_values &lt;- sapply(mk_results, function(x) x$sl)  # p-values\ntau_values &lt;- sapply(mk_results, function(x) x$tau)  # Tau values\n\n# Perform a chi-square test for heterogeneity\nchi_square_result &lt;- chisq.test(p_values)\nchi_square_result\n\n\n    Chi-squared test for given probabilities\n\ndata:  p_values\nX-squared = 0.23847, df = 4, p-value = 0.9934\n\n\nHowever, because how it performs trend test the values differ slightly. This is where knowing/learning how to lift the hood of the functions/packages helps. If you dig into the source code for the Kendall R-package you find that the functions within use a Fortran script with functions in it that differ slightly from how other packages do it. Not saying its wrong or right, just different.\nThe wql R-package has a tests for homogeneity of seasonal trends function and produces results identical to those produced by EnvStats.\n\n\nlibrary(wql)\n\nts(test2$timeseries$value,frequency = 12)|&gt;\n  trendHomog()\n\n$chi2.trend\n[1] 67.28\n\n$chi2.homog\n[1] 0.4\n\n$p.value\n[1] 0.9999996\n\n$n\n[1] 12\n\n\nIf you are curious like me and wonder how the chi squared is EXACTLY calculated here is the simplified code extracted (and slightly modified) from the wql::trendHomog function.\n\ntest2.df &lt;- dcast(test2$timeseries,Yr~Mon,value.var=\"value\",mean)\n\nvBH_hetero &lt;- function(x){\n  Sval &lt;- apply(x,2,FUN = function(y){\n    y &lt;- y\n    t &lt;- 1:length(y)\n    \n    outr &lt;- sign(outer(y, y, \"-\")/outer(t, t, \"-\"))\n    S &lt;- sum(outr[lower.tri(outr)],na.rm=T)\n    return(S)\n  })\n  \n  varS &lt;- apply(x,2,FUN = function(y){\n    ties &lt;- rle(sort(y))$lengths\n    n &lt;- length(y)\n    t1 &lt;- n * (n - 1) * (2 * n + 5)\n    t2 &lt;- sum(ties * (ties - 1) * (2 * ties + 5))\n    varS &lt;- (t1 - t2)/18\n    return(varS)\n  })\n  \n  fr &lt;- length(varS)\n  Z &lt;- Sval / sqrt(varS)\n  chi2.tot &lt;- sum(Z ^ 2)\n  Zbar &lt;- mean(Z)\n  chi2.trend &lt;- fr * Zbar ^ 2\n  chi2.homog &lt;- chi2.tot - chi2.trend\n  p.value &lt;- pchisq(chi2.homog, fr - 1, 0, FALSE)\n  \n  data.frame(chi2.trend = chi2.trend,\n             chi2.homog = chi2.homog,\n             p.value = p.value,\n             n = fr)\n}\n\n# Input data by removing the month column\nvBH_hetero(test2.df[,-1])\n\n  chi2.trend chi2.homog   p.value  n\n1      67.28        0.4 0.9999996 12"
  },
  {
    "objectID": "news/20241117_Timeseries3.html#other-kendall-tests",
    "href": "news/20241117_Timeseries3.html#other-kendall-tests",
    "title": "Time series Analysis (Part III), Welcome to the Kendall Family?",
    "section": "Other Kendall Tests",
    "text": "Other Kendall Tests\nThere are other Kendall tests but they are generally variants to the Mann-Kendall or seasonal Mann-Kendall including the Regional Kendall Tests which can also include seasonal tests. Other tests to include in the Kendall family are flow-adjusted trends and censored data (non-detects)."
  },
  {
    "objectID": "news/20241117_Timeseries3.html#usgs-software-vs-r-package",
    "href": "news/20241117_Timeseries3.html#usgs-software-vs-r-package",
    "title": "Time series Analysis (Part III), Welcome to the Kendall Family?",
    "section": "USGS software vs R-package",
    "text": "USGS software vs R-package\nAs mentioned at the start of this post, the USGS Kendall family of trend tests (link to info). Typically this software (and the code behind it) is used by some analyst to benchmark again other routines/code/methods. Much like how some estimates of a given statistical test can vary between packages based on the way things are calculated or how the different code infrastructure does the maths. That being said there is no direct translation or “R-version” of the USGS program (yet…saving it for a rainy day). For those interested, here is a workflow comparison between the USGS software and the various R-package.\n\n\n\nFeatureUSGS Kendall SoftwareR PackagesMann-Kendall TestBuilt-in`Kendall`, `trend`, `EnvStats`Seasonal Kendall TestBuilt-in`trend::seaKen`, `wq`Flow-Adjusted TrendsBuilt-inCustomizable (e.g., `mgcv`), `EGRET`Censored Data HandlingBuilt-in`NADA`, `NADA2`, `survival`Theil-Sen Slope EstimationBuilt-in`trend`, `mblm`, `wq`Autocorrelation AdjustmentBuilt-in (limited)`trend`, `zyp`VisualizationBasic`ggplot2`, Base R\n\n\nRemember (as discussed above) some R-packages calculate things slightly different. They shouldn’t be so different that it would changes the final outcome but enough to question how a particular statistic was calculated."
  },
  {
    "objectID": "news/20241026_Timeseries1.html",
    "href": "news/20241026_Timeseries1.html",
    "title": "Time series Analysis (Part I), the basics",
    "section": "",
    "text": "This blog post effectively breaks my blog writing dry streak. The last offical blog post (not stats related) was on my old blog platform (link) 1170 days ago! This post was motivated by a recent LinkedIn post by Joachim Schork about breaking down time series (see below and here) and the comments from the larger community. Its also motivated by a recent spur of time series decomposition analyses I’ve seen of late during meetings and discussions with colleagues."
  },
  {
    "objectID": "news/20241026_Timeseries1.html#definition",
    "href": "news/20241026_Timeseries1.html#definition",
    "title": "Time series Analysis (Part I), the basics",
    "section": "Definition",
    "text": "Definition\nFirst, what is time series data? If you look in the dictionary it will say something along the lines of a series of values/measurements/observations obtained at successive times, often (but not always) with equal intervals between them. Simply put its data collected over time. In my corner of science that could mean daily water levels, weekly total phosphorus concentrations, annual seagrass coverage, etc. Once collected these data can analyzed in a variety of ways depending on the motivation of why and where its being collected. Again, in my realm of the science, something we are interested in is the change in conditions overtime for a variety of reasons including (but not limited to) climate change, landscape scale changes (i.e. land-use alterations, dam removal, stream diversion, etc.), restoration activities, forecast modeling, etc. In this case, a time series analysis is needed to see how things are changing overtime."
  },
  {
    "objectID": "news/20241026_Timeseries1.html#formatting",
    "href": "news/20241026_Timeseries1.html#formatting",
    "title": "Time series Analysis (Part I), the basics",
    "section": "Formatting",
    "text": "Formatting\nWhen handling time series data in R you can either handle the data as a data.frame (or tibble if you are a tidyverse person … I personally am not, no judgement) or a ts object.\nGenerate some example value\n\nset.seed(123)\nvalues &lt;- runif(5)|&gt;\n  round(2)\n\n\n\n\ndata.frame\n\n\nts\n\n\n\n\ndata.frame(Yr = 2012:2016, \n           value = values\n           )\n\n    Yr value\n1 2012  0.29\n2 2013  0.79\n3 2014  0.41\n4 2015  0.88\n5 2016  0.94\n\n\n\nts(values, start = 2012)\n\nTime Series:\nStart = 2012 \nEnd = 2016 \nFrequency = 1 \n[1] 0.29 0.79 0.41 0.88 0.94\n\n\n\n\nThere are pros and cons of formatting the data either way. Most time series analysis functions can handle both in someway but most like it to be a ts object. The way the ts function works is it essentially converts the data into a special kind of data matrix (in R its called class or object) with an added header containing some information about the time series (like the example above). Depending on the information you include in the ts function it makes some assumptions, for instance with frequency = 12 it assumes its monthly data or frequency = 365 assumes daily.\n\nrnorm(24)|&gt;\n  round(2)|&gt;\n  ts(frequency = 12)\n\n    Jan   Feb   Mar   Apr   May   Jun   Jul   Aug   Sep   Oct   Nov   Dec\n1 -1.69  1.24 -0.11 -0.12  0.18  1.28 -1.73  1.69  0.50  2.53  0.55  0.24\n2 -1.05  1.29  0.83 -0.06 -0.78 -0.73 -0.22 -0.33 -1.09 -0.09  1.07 -0.15\n\n\nSee ?ts for more details on specifics. A very big drawback when using ts is if you are working with daily data with a leap year or years with leap years mixed in ts doesn’t know how to handle that extra day since the input is a vector or list of data. There might be a way to coerce it but I’ve yet to figure it out. Which is why I prefer to work in the data.frame world whenever possible."
  },
  {
    "objectID": "news/20241026_Timeseries1.html#synthetic-data",
    "href": "news/20241026_Timeseries1.html#synthetic-data",
    "title": "Time series Analysis (Part I), the basics",
    "section": "Synthetic Data",
    "text": "Synthetic Data\nAs an example this post (and the following series) rather than grabbing some random datasets as an example I wanted to use something that we have some knowledge on before digging into the specifics. This includes some of the components discussed above. To achieve this I put together some basic functions to simulate some data. The first function simulate_timeseries is relatively basic and simulate_timeseries_vol is a little more complex that includes some volatility and randomness factors.\n\nsimulate_timeseries &lt;- function(n = 1000,            # Number of observations\n                                trend_slope = 0.01,    # Slope of linear trend\n                                seasonal_amp = 2,      # Amplitude of seasonal component\n                                seasonal_period = 12,  # Period length\n                                noise_mean = 1,        # mean of noise\n                                noise_sd = 0.5,        # Standard deviation of noise  \n                                seed.val = 123         # value to set.seed(...)\n){         \n  if(is.null(seed.val)==F){set.seed(seed.val)} # for reproducibility\n  # Generate time points\n  t &lt;- 1:n\n  # Create components\n  # 1. Linear trend\n  trend &lt;- trend_slope * t\n  # 2. Seasonal component using sine wave\n  seasonal &lt;- seasonal_amp * sin(2 * pi * t / seasonal_period)\n  # 3. Random noise (stationary component)\n  noise &lt;- rnorm(n, mean = noise_mean, sd = noise_sd)\n  # Combine components\n  ts_data &lt;- trend + seasonal + noise\n  \n  # Convert to time series object\n  ts_result &lt;- ts(ts_data, frequency = seasonal_period)\n  # Return both the time series and its components for analysis\n  return(list(\n    timeseries = data.frame(time = t, value = as.numeric(ts_result)),\n    components = list(\n      trend = trend,\n      seasonal = seasonal,\n      noise = noise\n    )\n  ))\n}\n\nsimulate_timeseries_vol &lt;- function(n = 1000,              # Number of observations\n                                    trend_slope = 0.01,    # Slope of linear trend\n                                    seasonal_amp = 2,      # seasonal component\n                                    seasonal_period = 12,  # Period length\n                                    init_vol = 0.5,        # Initial volatility\n                                    vol_persistence = 0.95,# Persistence in volatility\n                                    rw_sd = 0.1,           # Random walk innovation SD\n                                    seed.val = 123         # value to set.seed(...)  \n){         \n  if(is.null(seed.val)==F){set.seed(seed.val)} # for reproducibility\n  # Generate time points\n  t &lt;- 1:n\n  # 1. Create non-linear trend (combining linear trend with random walk)\n  linear_trend &lt;- trend_slope * t\n  random_walk &lt;- cumsum(rnorm(n, 0, rw_sd))\n  trend &lt;- linear_trend + random_walk\n  # 2. Create time-varying seasonal component\n  # Amplitude changes over time following a random walk\n  varying_amplitude &lt;- seasonal_amp + cumsum(rnorm(n, 0, rw_sd/5))\n  seasonal &lt;- varying_amplitude * sin(2 * pi * t / seasonal_period)\n  # 3. Generate time-varying volatility (GARCH-like process)\n  # Initialize volatility vector\n  volatility &lt;- numeric(n)\n  volatility[1] &lt;- init_vol\n  # Generate volatility process\n  for(i in 2:n) {\n    # Volatility follows AR(1) process with innovations\n    volatility[i] &lt;- sqrt(0.01 + \n                            vol_persistence * volatility[i-1]^2 + \n                            (1 - vol_persistence) * rnorm(1, 0, 0.1)^2)\n  }\n  # 4. Generate heteroskedastic noise\n  noise &lt;- rnorm(n, 0, 1) * volatility\n  # 5. Add structural breaks\n  # Add random level shifts\n  n_breaks &lt;- max(1, round(n/200))  # Approximately one break every 200 observations\n  break_points &lt;- sort(sample(2:n, n_breaks))\n  level_shifts &lt;- cumsum(rnorm(n_breaks, 0, 2))  # Random shift magnitudes\n  breaks &lt;- numeric(n)\n  current_break &lt;- 1\n  for(i in 1:n) {\n    if(current_break &lt;= length(break_points) && i &gt;= break_points[current_break]) {\n      breaks[i:n] &lt;- level_shifts[current_break]\n      current_break &lt;- current_break + 1\n    }\n  }\n  \n  # Combine all components\n  ts_data &lt;- trend + seasonal + noise + breaks\n  # Convert to time series object\n  ts_result &lt;- ts(ts_data, frequency = seasonal_period)\n  # Return both the time series and its components\n  return(list(\n    timeseries = data.frame(time = t, value = as.numeric(ts_result)),\n    components = list(\n      trend = trend,\n      seasonal = seasonal,\n      noise = noise,\n      breaks = breaks,\n      volatility = volatility\n    )\n  ))\n}\n\nHere are some example simulated datasets. Lets assume this data is monthly for a 15 year period of record.\n\nn.vals &lt;- 15*12 # 15 years 12 months per year \ndat1 &lt;- simulate_timeseries(\n  n = n.vals,           # About 15 years of monthly data\n  trend_slope = 0.025,  # Upward trend\n  seasonal_amp = 2,     # Seasonal fluctuation of ±2\n  seasonal_period = 12, # Monthly seasonality\n  noise_sd = 0.5,       # Moderate noise\n  noise_mean = 3\n)\n## add some years and months\ndat1$timeseries  &lt;-  cbind(dat1$timeseries,\n                           expand.grid(Mon = 1:12, Yr = 1990:2004)\n)\n\ndat2 &lt;- simulate_timeseries_vol(\n  n = n.vals,           # About 15 years of daily data\n  trend_slope = 0.05,   # Upward trend\n  seasonal_amp = 2,     # Base seasonal amplitude\n  seasonal_period = 10, # Monthly seasonality\n  init_vol = 0.5,      # Initial volatility\n  vol_persistence = 0.65,\n  rw_sd = 0.3\n)\n## add some years and months\ndat2$timeseries  &lt;-  cbind(dat2$timeseries,\n                           expand.grid(Mon = 1:12, Yr = 1990:2004)\n)\n\ndat3 &lt;- simulate_timeseries(\n  n = n.vals,           # About 15 years of monthly data\n  trend_slope = 0.001,  # no trend\n  seasonal_amp = 2,     # Seasonal fluctuation of ±2\n  seasonal_period = 12, # Monthly seasonality\n  noise_sd = 0.5,       # Moderate noise\n  noise_mean = 3\n)\n\ndat4 &lt;- simulate_timeseries(\n  n = n.vals,           # About 15 years of monthly data\n  trend_slope = -0.05,  # downward trend\n  seasonal_amp = 2,     # Seasonal fluctuation of ±2\n  seasonal_period = 12, # Monthly seasonality\n  noise_sd = 0.5,       # Moderate noise\n  noise_mean = 15\n)\n\nHere is a quick plot of the data …\n\n\n\n\n\nQuick plots of example data"
  },
  {
    "objectID": "news/20241026_Timeseries1.html#basic-trend-analysis",
    "href": "news/20241026_Timeseries1.html#basic-trend-analysis",
    "title": "Time series Analysis (Part I), the basics",
    "section": "Basic Trend Analysis",
    "text": "Basic Trend Analysis\nA simple trend test would look like this … For simplicity sake we are going to use time as its numeric but hypothetically decimal date (see lubridate::decimal_date for more info) is also an option.\n\ncor.test(dat1$timeseries$time,dat1$timeseries$value,method = \"kendall\")\n\n\n    Kendall's rank correlation tau\n\ndata:  dat1$timeseries$time and dat1$timeseries$value\nz = 8.7492, p-value &lt; 2.2e-16\nalternative hypothesis: true tau is not equal to 0\nsample estimates:\n     tau \n0.438982 \n\ncor.test(dat2$timeseries$time,dat2$timeseries$value,method = \"kendall\")\n\n\n    Kendall's rank correlation tau\n\ndata:  dat2$timeseries$time and dat2$timeseries$value\nz = 11.996, p-value &lt; 2.2e-16\nalternative hypothesis: true tau is not equal to 0\nsample estimates:\n      tau \n0.6018622 \n\n\nBased on the plots you would have guessed that the datasets had an increasing trend. But here is what no-trend (dat3) and negative trend (dat4) looks like.\n\n\n\n\n\nQuick plots of example data\n\n\n\n\n\ncor.test(dat3$timeseries$time,dat3$timeseries$value,method = \"kendall\")\n\n\n    Kendall's rank correlation tau\n\ndata:  dat3$timeseries$time and dat3$timeseries$value\nz = -0.24248, p-value = 0.8084\nalternative hypothesis: true tau is not equal to 0\nsample estimates:\n        tau \n-0.01216636 \n\ncor.test(dat4$timeseries$time,dat4$timeseries$value,method = \"kendall\")\n\n\n    Kendall's rank correlation tau\n\ndata:  dat4$timeseries$time and dat4$timeseries$value\nz = -13.708, p-value &lt; 2.2e-16\nalternative hypothesis: true tau is not equal to 0\nsample estimates:\n       tau \n-0.6877716 \n\n\nSometimes we are more interested in long term trends … be careful when using “long” and “short” these are usually very context specific and care should be taken to delineate/define as much as possible. For sake of brevity lets look at an annual trend analysis.\nFirst, the data needs to be aggregated:\n\ndat1_yr &lt;- aggregate(value~Yr,dat1$timeseries,mean)\n\nYou can also look at the annual variability or standard deviation using aggregate.\n\naggregate(value~Yr,dat1$timeseries,sd)\n\n     Yr    value\n1  1990 1.631108\n2  1991 1.674150\n3  1992 1.277267\n4  1993 1.359000\n5  1994 1.481055\n6  1995 1.197387\n7  1996 1.442100\n8  1997 1.458378\n9  1998 1.646581\n10 1999 1.457668\n11 2000 1.455360\n12 2001 1.447669\n13 2002 1.428132\n14 2003 1.460503\n15 2004 1.533227\n\n\n\n\n\n\n\nQuick plots of aggregated data from dat1.\n\n\n\n\nOr do something more elaborate to calculate all sorts of statistics. My preference (again not tidyverse) is functionality in the plyr package.\n\nlibrary(plyr)\nddply(dat1$timeseries,c(\"Yr\"),summarise,\n      mean.val = mean(value),\n      sd.val = sd(value),\n      var.val = var(value),\n      N.val = length(value), #or AnalystHelper::N.obs(value)\n      SE = AnalystHelper::SE(value),\n      med.val = median(value),\n      min.val = min(value),\n      max.val = max(value))|&gt;\n  round(2)# added to consolidate the table down\n\n     Yr mean.val sd.val var.val N.val   SE med.val min.val max.val\n1  1990     3.26   1.63    2.66    12 0.47    3.61    0.84    5.85\n2  1991     3.36   1.67    2.80    12 0.48    3.03    0.99    6.03\n3  1992     3.85   1.28    1.63    12 0.37    4.20    1.92    6.09\n4  1993     4.02   1.36    1.85    12 0.39    3.96    1.86    5.82\n5  1994     4.48   1.48    2.19    12 0.43    4.82    1.65    6.40\n6  1995     4.60   1.20    1.43    12 0.35    4.42    2.99    6.41\n7  1996     4.96   1.44    2.08    12 0.42    4.88    3.03    7.14\n8  1997     5.44   1.46    2.13    12 0.42    5.46    3.30    7.72\n9  1998     5.48   1.65    2.71    12 0.48    5.27    3.15    7.95\n10 1999     5.75   1.46    2.12    12 0.42    5.66    3.90    7.94\n11 2000     6.20   1.46    2.12    12 0.42    6.26    3.74    8.05\n12 2001     6.29   1.45    2.10    12 0.42    6.56    4.05    8.70\n13 2002     6.70   1.43    2.04    12 0.41    6.46    4.61    8.78\n14 2003     7.18   1.46    2.13    12 0.42    6.97    4.92    9.46\n15 2004     7.42   1.53    2.35    12 0.44    7.79    5.12    9.17\n\n\nOnce the data is aggregated we can use the simple Kendall test to look at the overall annual trend in the data.\n\ncor.test(dat1_yr$Yr,dat1_yr$value,method = \"kendall\")\n\n\n    Kendall's rank correlation tau\n\ndata:  dat1_yr$Yr and dat1_yr$value\nT = 105, p-value = 1.529e-12\nalternative hypothesis: true tau is not equal to 0\nsample estimates:\ntau \n  1 \n\n\nLooks like a perfect increasing trend! The Kendall-𝜏 is a type of correlation coefficient so the values should range between -1 and 1. Based on the 𝜌-value, (assuming an 𝛼-value of 0.05) this is a statistically significant trend. (Lets not get into the 𝜌-value discussion!)\nWe can also look at the rate of change. The most common is the Theil-Sen slope estimator. There are several different packages out there that can estimate Theil-Sen slope but the most basic one that I’ve come across is the median based linear model or mblm package. We can discuss the basics of Theil-Sen and mblm in a later post but after some digging and search what I’ve determined is that Theil-Sen method and median-based linear model are closely related. Generally, the Theil-Sen method is a non-parametric method that calculates the median of slopes of all lines through the data and the intercept is often the median of the residuals after fitting the slope. Meanwhile, the median-based linear model is a regression model that uses medians as a criterion for fitting a model and minimizes the sum of absolute residuals. In most cases, the Theil-Sen slope and the slope determined by the median-based linear model are the same with minuscule differences. Most trend routines use median-based linear models or quantile regressions as a basis to perform Theil-Sen estimation.\nIf you run the side-by-side of mblm, zyp and a separate Theil-Sen estimator pulled from the litature (found here) you’ll undoubtly come to similar, if not the same values. Don’t believe me lets give it a try.\n\nlibrary(zyp)\nlibrary(mblm)\n\n# From University of Virigina function (from literature)\ntheil_sen &lt;- function(x,y){\n  n &lt;- length(x)\n  max_n_slopes &lt;- (n * (n - 1)) / 2\n  slopes &lt;- vector(mode = 'list', length = max_n_slopes) # list of NULL elements\n  add_at &lt;- 1\n  # Calculate point-to-point slopes if x_i != x_j\n  for (i in 1:(n - 1)) {\n    slopes_i &lt;- lapply((i + 1):n, function(j) \n      if (x[j] != x[i]) { (y[j] - y[i]) / (x[j] - x[i]) })\n    slopes[add_at:(add_at + length(slopes_i) - 1)] &lt;- slopes_i\n    add_at &lt;- add_at + length(slopes_i)\n  }\n  # Calculate Theil-Sen slope\n  slopes &lt;- unlist(slopes) # drop NULL elements\n  theil_sen_slope &lt;- median(slopes, na.rm = TRUE)\n  # Calculate Theil-Sen intercept\n  intercepts &lt;- y - (theil_sen_slope * x)\n  theil_sen_intercept &lt;- median(intercepts, na.rm = TRUE)\n  # Return\n  c('Theil-Sen intercept' = theil_sen_intercept, \n    'Theil-Sen slope' = theil_sen_slope)\n}\n\n\nset.seed(123)\n\n# Generate linear data\nx &lt;- 1:100\ny &lt;- 2*x + rnorm(100, 0, 10)\n# Add some outliers\ny[c(10, 30, 50, 70, 90)] &lt;- y[c(10, 30, 50, 70, 90)] + 50\n\nplot(y~x,las=1)\n\n\n\nzyp.sen &lt;-  zyp.sen(y~x); # zyp package\nmblm.sen &lt;-  mblm(y~x,repeated = F); # mblm package\nUV.sen &lt;-  theil_sen(x,y); # University of Virigina function (from literature)\n\n\n\n  Pack intercept   Slope\n1  zyp   0.40106 2.01976\n2 mblm   0.40106 2.01976\n3   UV   0.40106 2.01976\n\n\nThere you have it, all three methods produce the same estimates.\nBack to our aggregated dataset, the code is pretty straight forward but there are a couple of things to be aware. If you’ve fit linear models using lm its the same general format. However, currently the mblm function can’t handle datasets with NAs (unlike lm), so you might have to clean up the data a little (i.e. na.omit() can be your friend). Also double check that both variables are numeric.\n\nlibrary(mblm)\n\ndat1.theilsen &lt;- mblm(value~Yr,dat1_yr,repeated=F)\n\ndat1.theilsen\n\n\nCall:\nmblm(formula = value ~ Yr, dataframe = dat1_yr, repeated = F)\n\nCoefficients:\n(Intercept)           Yr  \n  -587.9833       0.2971  \n\n# extrapolate example\npredict(dat1.theilsen,data.frame(Yr=2005:2014),interval =\"confidence\")\n\n         fit       lwr       upr\n1   7.629230  7.500516  7.757944\n2   7.926294  7.784963  8.067624\n3   8.223357  8.069143  8.377572\n4   8.520421  8.353116  8.687725\n5   8.817484  8.636929  8.998040\n6   9.114548  8.920614  9.308482\n7   9.411611  9.204195  9.619028\n8   9.708675  9.487692  9.929658\n9  10.005739  9.771119 10.240358\n10 10.302802 10.054488 10.551116\n\n\nThe Theil-Sen estimator for the annually aggregated data is 0.297 units per year. This is the rate of annual change. If this was something like sea-level rise, rainfall rate, population growth, etc. you can do a very basic analysis and ask at this rate what would the values be in 10 years (extrapolating … 10.3 units) or in the case of sea-level rise whats the rise per decade (2.97 units per decade)?"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "\nWelcome to the swamp\n",
    "section": "",
    "text": "Welcome to the swamp\n\n\nI am an ecologist with a multi-disciplinary background focused on aquatic biogeochemistry and ecology at the landscape scale. Generally, I can be classified as an aquatic biogeochemist studying the transformation of nutrients and other compounds along the freshwater to marine aquatic continuum. I enjoy studying all aquatic ecosystems but wetlands hold a special place in my heart.\nA couple of words/phrases to describe me: Wetland Biogeochemist, Ecologist, Data-scientist, Soil-scientist, Naturalist, Swamp Walker, Lover of all things R.\nThis website will host some general information about myself, my research interests and publications.\n#rstats #python #geospatial #GIS #soil #ecology #climatechange #biogeochemistry #wetland #aquatic\n\n\n\n\nSelected Publications\nGoogle Scholar Citations = 359  H-Index = 11  i10-Index = 13 \n\nJulian et al (2024) Dark waters: Evaluating seagrass community response to optical water quality and freshwater discharges in a highly managed subtropical estuary. Regional Studies in Marine Science 69:103302.\nJulian et al (2023) Changes in the spatial distribution of total phosphorus in sediment and water column of a shallow subtropical lake. Lake and Reservoir Management 39:213–230.\nJulian et al (2022) Understanding the ups and downs, application of hydrologic restoration measures for a large Subtropical Lake. Lake and Reservoir Management 38:304–317.\nSmith MC, Julian P, DeAngelis D, Zhang B (2023) Ecological benefits of integrative weed management of Melaleuca quinquenervia in Big Cypress National Preserve. BioControl.\n\n\n\n\n\n\nR Packages\nEPGMr   LimnoPalettes  NADA2  AnalystHelper   LORECOVER  CalSalMod \n\n\n\n\n\n\n\n\nWords of inspiration\n\n“My temple is the swamp …When I would recreate myself, I seek the darkest wood, the thickest and most impenetrable and to the citizen, most dismal, swamp. I enter a swamp as a sacred place, a sanctum sanctorum… I seemed to have reached a new world, so wild a place …far away from human society. What’s the need of visiting far-off mountains and bogs, if a half-hour’s walk will carry me into such wildness and novelty.”\nHenry David Thoreau, Walden and Other Writings\n“Whatever you want to do in this world, it is achievable. The most important thing that I’ve found, that perhaps you could use, is be passionate and enthusiastic in the direction that you choose in life, and you’ll be a winner.”\nSteve Irwin, Crikey! What an Adventure\n“…One thing you quickly sense about the people here: they include more than average proportion of idealists and self-motivated seekers. But they’re all pretty down-to-earth. The work demands a lot of physical and mental effort. Slackers don’t make it this far.”\nCarl Safina, Eye of the Albatross\n“I remember a hundred lovely lakes, and recall the fragrant breath of pine and fir and cedar and poplar trees. The trail has strung upon it, as upon a thread of silk, opalescent dawns and saffron sunsets. It has given me blessed release from care and worry and the troubled thinking of our modern day. It has been a return to the primitive and the peaceful. Whenever the pressure of our complex city life thins my blood and benumbs my brain, I seek relief in the trail; and when I hear the coyote wailing to the yellow dawn, my cares fall from me - I am happy.”\nHamlin Garland “Hitting the Trail” - 1899\n\n\n\n\n\n\n\n\nAuthorship Guidelines"
  },
  {
    "objectID": "Authorship.html",
    "href": "Authorship.html",
    "title": "Authorship Agreement",
    "section": "",
    "text": "1Adapted from the Saltmarsh Habitat & Avian Research Program (https://www.tidalmarshbirds.org/) Guidelines for Authorship Standard Operating Procedure.\n\nDeciding authorship on scientific publications can be complicated because practices and cultural norms vary across disciplines and even across labs within the same discipline. This is especially relevant in ecology, where standardized guidelines are lacking and a diversity of options exist for deciding authorship and author order. These guidelines outline a set of criteria for authorship determinations. These criteria are presented as guidelines, because a common set of expectations is important to maintain mutual satisfaction among co- authors. We recognize, however, that some flexibility will be required and communication is essential to the process. Before all else, remember conversations regarding authorship for each manuscript should happen early, frequently, openly, and inclusively. A conversation should be expected when the paper is first conceived and should be revisited periodically as each project develops.\n\n\nAuthorship on a manuscript is warranted when a researcher has made a substantial contribution to the manuscript in question (not the overall project as a whole), as defined by any two of the following:\n\nConceiving of ideas and/or study design and/or analytical approach\nWriting of the manuscript (or sections)\nReviewing and editing the manuscript\nAnalyzing data\nInterpreting results\nCollecting data in field or lab (except in rare circumstances this will not include temporary technicians)\nCreating or managing critical databases (e.g. demographic database, historical abundance estimates spreadsheet)\nObtaining funding (e.g., proposal writing, grant management, and project reports)\n\nOther things are important to keep in mind (expanded upon in the sections below).\n\nA conversation is necessary for each manuscript\nConsider thresholds of effort (“could the study have been done without the contribution?”)\nOffer further involvement\nUse inclusion to deal with uncertainty (“better to be inclusive than to exclude”)\nPrimary authors and their advisors/PIs/mentors will make the final decision on authorship\nInvite prior individuals (alumni) to participate when appropriate\nThere are exceptions for grant deliverables.\nWhen in doubt, talk it out!\n\nConsider Thresholds of Effort\nUltimately, the primary author must have some leeway in making authorship decisions, and ensuring that a certain minimum threshold of contribution has been made. When the level of the contribution to the particular manuscript is unclear (e.g. as in the case of data collection), the deciding question becomes “could the study have been done without that person’s contribution?”\nFor instance, did the extra work amount to a few data points within a huge dataset (if the analysis was enhanced by their participation, but was possible without it, authorship may not be warranted), or were the data points critical to establishing the pattern (if the analysis is impossible without the data from this study site, or if trends depend on those data, authorship is more clearly warranted). Other considerations: If a researcher is collecting data for a study that is not their own, did they do extra work that they would not otherwise have done on their own study site or for their own study (if so, the case for authorship increases)? Did the effort amount to a few days of fieldwork (may not warrant authorship) or a season’s worth of logistics and data collection (more clearly warrants authorship)?\nOffer Further Involvement\nIn some instances, a contributor may have clearly passed a threshold of effort (see previous), but will have only contributed to one of the categories that would qualify them for consideration of authorship. In these instances, it is the primary author’s responsibility to reach out to the contributor early in the writing process and have a conversation about further involvement. The second category can be easily achieved by assistance with developing and reviewing the manuscript, and contributors that have clearly passed a threshold of effort should be given that opportunity.\nUse Inclusion to Deal with Uncertainty\nRecognize that the contribution of effort is a gradient with clear endpoints (one data point out of two probably gets you authorship, one data point out of 1,000 probably doesn’t), so there will likely be situations where it is unclear (200 data points out of 1,000?). If there is any uncertainty in gauging the contribution, it is better to be inclusive than to exclude, and it is better to talk directly to the contributor explicitly. A quick phone call made in a spirit of inclusion can almost always improve the situation for everyone, both for this manuscript and for future collaboration.\nInvite Alumni to Participate when Appropriate\nAs data collected by others who have moved on are used in analysis, we need to give them credit for their prior work. If the data have already been published in another form and their papers can be cited, this may be enough. Authorship may be warranted or offered, however, under several circumstances. First, if the individual was involved in the conception of the ideas in the new manuscript, this would warrant their inclusion. Second, if the new manuscript is based largely on the data (or conceptual groundwork) of a single individual (similar to the rules for contemporary contributors, could the analysis be completed without their data?), authorship should be considered. If the previous work of prior individuals passes the Threshold of Effort test for any reason, the burden is on the PI involved to offer the opportunity for further engagement in the new manuscript to new individuals, preferably early in the process. If prior individuals respond positively and stay involved, then they should be authors on the new work. Prior individuals are responsible for deciding to stay engaged and following through with their involvement. Importantly, prior individuals should understand that if they do not respond to inquiries about authorship, historical datasets can still be used, but they will not be included as authors. This same approach may be followed for the advisors of prior individuals if they pass the Threshold.\n\n\n\nThe order of authors on publications will follow the practice of first-last author emphasis. The first author will be the person who did the majority of the work, carried out the study, and will often be a the primary writer of the manuscript. Typically, the last author should be the lab PI. “Credit” or “importance” is attributed to authors in the following order: first, last, 2nd, 3rd, 4th, etc. Final decisions regarding author order ultimately lie with the first author, in consultation with their advisor, PI or mentor. Authors that disagree with the draft author order, however, should feel comfortable voicing their concerns. More importantly, primary authors should ask specifically for co-authors to approve the final order via email.\nAnyone listed as an author must be given a fair opportunity to read and comment on the manuscript. If prospective authors do not respond in a reasonable amount of time, they should be removed, barring exceptional circumstances (the response could be as simple as “manuscript is good to go”, as long as the author acknowledges and approves the content). The lead author should give AT LEAST two weeks for the response period and should specify a date by which comments are due. Two corollaries of this guideline are 1) anyone has the right to request removal as author from a paper for any reason, including a personal judgment of failure to cross the Threshold for Effort, and 2) no one should ever be an author on a manuscript where they did not approve the final submitted draft (note that many journals have this requirement).\nIt is the primary author’s responsibility, as corresponding author, to provide all co-authors with:\n\nA digital version of the final submitted draft\nNews of all significant correspondence with the editor/publisher\nThe opportunity to assist with revisions\nA digital version of all revisions submitted for publication and the responses to reviewers\nPage proofs and the opportunity to comment on them\nA final pdf of all published papers\n\n\n\n\nExpectations for authorship are a set of evolving cultural norms. This means that 1) they must be taught anew to each set of students and/or new individual early in their involvement with the project and 2) the guidelines in this document need to be revisited and updated regularly (~annually or as needed).\n\n\n\n\n\nThe dissemination of research to the broader scientific community is not only part of the research process, but perhaps the ultimate reason for conducting research in the first place. There is an expectation and responsibility to share one’s work with the broader community (Cooke et al. 2014). It goes without saying that composing a manuscript by committee can be difficult. Co-authorship comes about in a variety of ways some of which may not require a formal role in writing but rather the collection of data, editing of the manuscript, etc. Some manuscripts are planned in advanced and have the luxury of identfying the core author pool. Other manuscripts are produced ad-hoc. Regardless of the origin it is recommended that if the manuscript has more than two authors it is advisable that a lead author along with a supporting person (typically senior author assumes this role) be identified. The objective of having two (or more) authors is to have these individuals generate a paper that is as close to final as possible, after all two-eyes are better than one. Letting other co-authors know that the paper is ready to move forward in the eyes of both the lead senior authors will often help to elicit a rapid review and response by co-authors.\nEveryone is busy, therefore some degree of time-management is needed when composing an manuscript. Therefore it is recommended that tentative deadlines/time-periods are discussed early and often. If necessary these deadline can be revised as needed due to changing priorities within reason but make sure these changes are communicated amongst all authors. Once a draft manuscript is produced and ready for co-author input provide a reasonable review deadline (couple of weeks). Typically, if co-authors are unresponsive its not they don’t find the manuscript important. It is possible that it has been shuffled down the dreaded “to do” list or they just forgot. This has happened to all of us at some point. Therefore a simple reminder would be helpful to re-engage the co-authors. Below is a flow chart from Cooke et al. (2014) with a suggested example of sequence of events to work with tardy or non-responsive co-authors.\n\n\n\n\n\nFlow-diagram with a possile sequence of events for working with tardy or non-responsive co-authors when preparing a manuscript for submission to a journal. From Cooke et al. (2014).\n\n\n\n\nAs suggested above, if co-authors are completely unresponsive it is the lead authors progrative, with input from co-authors to remove unresponsive co-authors. The Committee on Publication Ethics (COPE), has additional guidance information regarding authorship and ethics.\n\n\n\n\nJournals are increasingly requiring authors to specify roles according to author contributions. The days of honorary authorship is slowly (too slowly) coming to an end. However, there are many other aspects of research that are not adequately captured by the generally recognized short list of contributing roles, particularly in applied environmental disciplines such as conservation science, environmental science and applied ecology (herein referred to as conservation science). Cooke et al. (2021) provides some additional context and consideration to authorship guidelines reviewing existing and proposing potential expanded contributors roles.\nBottom line\nPoints summarized from the abstract of Cook et al. (2021).\n\nAuthorship should acknowledge and reward those deserving of credit. Being an author on a paper also means that one assumes some degree of ownership of the content, regardless of the authorship order. (Something, it feels, has been forgotten recently)\nThe growing recognition that authorship should reflect contributions that extend beyond the usual data collection, analysis and writing provides the ideal backdrop for rethinking contributions in conservation science. Cook et al. (2021) propose a more inclusive approach to authorship that recognizes and values diverse contributions and contributors using an expanded list of defined roles.\n\n\n\n\n\n\nCooke SJ, Donaldson MR, Clark TD (2014) Practical guidance for early career researchers dealing with tardy or unresponsive co-authors. Ideas in Ecology and Evolution. 1(7). Link.\nCooke SJ, Nguyen VM, Young N, et al (2021) Contemporary authorship guidelines fail to recognize diverse contributions in conservation science research. Ecological Solutions and Evidence 2:e12060. Link.\nSaltmarsh Habitat & Avian Research Program (2014) Guidelines for authorship. Saltmarsh Habitat & Avian Research Program. https://www.tidalmarshbirds.org/."
  },
  {
    "objectID": "Authorship.html#authorship-criteria",
    "href": "Authorship.html#authorship-criteria",
    "title": "Authorship Agreement",
    "section": "",
    "text": "Authorship on a manuscript is warranted when a researcher has made a substantial contribution to the manuscript in question (not the overall project as a whole), as defined by any two of the following:\n\nConceiving of ideas and/or study design and/or analytical approach\nWriting of the manuscript (or sections)\nReviewing and editing the manuscript\nAnalyzing data\nInterpreting results\nCollecting data in field or lab (except in rare circumstances this will not include temporary technicians)\nCreating or managing critical databases (e.g. demographic database, historical abundance estimates spreadsheet)\nObtaining funding (e.g., proposal writing, grant management, and project reports)\n\nOther things are important to keep in mind (expanded upon in the sections below).\n\nA conversation is necessary for each manuscript\nConsider thresholds of effort (“could the study have been done without the contribution?”)\nOffer further involvement\nUse inclusion to deal with uncertainty (“better to be inclusive than to exclude”)\nPrimary authors and their advisors/PIs/mentors will make the final decision on authorship\nInvite prior individuals (alumni) to participate when appropriate\nThere are exceptions for grant deliverables.\nWhen in doubt, talk it out!\n\nConsider Thresholds of Effort\nUltimately, the primary author must have some leeway in making authorship decisions, and ensuring that a certain minimum threshold of contribution has been made. When the level of the contribution to the particular manuscript is unclear (e.g. as in the case of data collection), the deciding question becomes “could the study have been done without that person’s contribution?”\nFor instance, did the extra work amount to a few data points within a huge dataset (if the analysis was enhanced by their participation, but was possible without it, authorship may not be warranted), or were the data points critical to establishing the pattern (if the analysis is impossible without the data from this study site, or if trends depend on those data, authorship is more clearly warranted). Other considerations: If a researcher is collecting data for a study that is not their own, did they do extra work that they would not otherwise have done on their own study site or for their own study (if so, the case for authorship increases)? Did the effort amount to a few days of fieldwork (may not warrant authorship) or a season’s worth of logistics and data collection (more clearly warrants authorship)?\nOffer Further Involvement\nIn some instances, a contributor may have clearly passed a threshold of effort (see previous), but will have only contributed to one of the categories that would qualify them for consideration of authorship. In these instances, it is the primary author’s responsibility to reach out to the contributor early in the writing process and have a conversation about further involvement. The second category can be easily achieved by assistance with developing and reviewing the manuscript, and contributors that have clearly passed a threshold of effort should be given that opportunity.\nUse Inclusion to Deal with Uncertainty\nRecognize that the contribution of effort is a gradient with clear endpoints (one data point out of two probably gets you authorship, one data point out of 1,000 probably doesn’t), so there will likely be situations where it is unclear (200 data points out of 1,000?). If there is any uncertainty in gauging the contribution, it is better to be inclusive than to exclude, and it is better to talk directly to the contributor explicitly. A quick phone call made in a spirit of inclusion can almost always improve the situation for everyone, both for this manuscript and for future collaboration.\nInvite Alumni to Participate when Appropriate\nAs data collected by others who have moved on are used in analysis, we need to give them credit for their prior work. If the data have already been published in another form and their papers can be cited, this may be enough. Authorship may be warranted or offered, however, under several circumstances. First, if the individual was involved in the conception of the ideas in the new manuscript, this would warrant their inclusion. Second, if the new manuscript is based largely on the data (or conceptual groundwork) of a single individual (similar to the rules for contemporary contributors, could the analysis be completed without their data?), authorship should be considered. If the previous work of prior individuals passes the Threshold of Effort test for any reason, the burden is on the PI involved to offer the opportunity for further engagement in the new manuscript to new individuals, preferably early in the process. If prior individuals respond positively and stay involved, then they should be authors on the new work. Prior individuals are responsible for deciding to stay engaged and following through with their involvement. Importantly, prior individuals should understand that if they do not respond to inquiries about authorship, historical datasets can still be used, but they will not be included as authors. This same approach may be followed for the advisors of prior individuals if they pass the Threshold."
  },
  {
    "objectID": "Authorship.html#author-order-and-etiquette",
    "href": "Authorship.html#author-order-and-etiquette",
    "title": "Authorship Agreement",
    "section": "",
    "text": "The order of authors on publications will follow the practice of first-last author emphasis. The first author will be the person who did the majority of the work, carried out the study, and will often be a the primary writer of the manuscript. Typically, the last author should be the lab PI. “Credit” or “importance” is attributed to authors in the following order: first, last, 2nd, 3rd, 4th, etc. Final decisions regarding author order ultimately lie with the first author, in consultation with their advisor, PI or mentor. Authors that disagree with the draft author order, however, should feel comfortable voicing their concerns. More importantly, primary authors should ask specifically for co-authors to approve the final order via email.\nAnyone listed as an author must be given a fair opportunity to read and comment on the manuscript. If prospective authors do not respond in a reasonable amount of time, they should be removed, barring exceptional circumstances (the response could be as simple as “manuscript is good to go”, as long as the author acknowledges and approves the content). The lead author should give AT LEAST two weeks for the response period and should specify a date by which comments are due. Two corollaries of this guideline are 1) anyone has the right to request removal as author from a paper for any reason, including a personal judgment of failure to cross the Threshold for Effort, and 2) no one should ever be an author on a manuscript where they did not approve the final submitted draft (note that many journals have this requirement).\nIt is the primary author’s responsibility, as corresponding author, to provide all co-authors with:\n\nA digital version of the final submitted draft\nNews of all significant correspondence with the editor/publisher\nThe opportunity to assist with revisions\nA digital version of all revisions submitted for publication and the responses to reviewers\nPage proofs and the opportunity to comment on them\nA final pdf of all published papers"
  },
  {
    "objectID": "Authorship.html#a-living-document",
    "href": "Authorship.html#a-living-document",
    "title": "Authorship Agreement",
    "section": "",
    "text": "Expectations for authorship are a set of evolving cultural norms. This means that 1) they must be taught anew to each set of students and/or new individual early in their involvement with the project and 2) the guidelines in this document need to be revisited and updated regularly (~annually or as needed)."
  },
  {
    "objectID": "Authorship.html#duty-as-a-co-author",
    "href": "Authorship.html#duty-as-a-co-author",
    "title": "Authorship Agreement",
    "section": "",
    "text": "The dissemination of research to the broader scientific community is not only part of the research process, but perhaps the ultimate reason for conducting research in the first place. There is an expectation and responsibility to share one’s work with the broader community (Cooke et al. 2014). It goes without saying that composing a manuscript by committee can be difficult. Co-authorship comes about in a variety of ways some of which may not require a formal role in writing but rather the collection of data, editing of the manuscript, etc. Some manuscripts are planned in advanced and have the luxury of identfying the core author pool. Other manuscripts are produced ad-hoc. Regardless of the origin it is recommended that if the manuscript has more than two authors it is advisable that a lead author along with a supporting person (typically senior author assumes this role) be identified. The objective of having two (or more) authors is to have these individuals generate a paper that is as close to final as possible, after all two-eyes are better than one. Letting other co-authors know that the paper is ready to move forward in the eyes of both the lead senior authors will often help to elicit a rapid review and response by co-authors.\nEveryone is busy, therefore some degree of time-management is needed when composing an manuscript. Therefore it is recommended that tentative deadlines/time-periods are discussed early and often. If necessary these deadline can be revised as needed due to changing priorities within reason but make sure these changes are communicated amongst all authors. Once a draft manuscript is produced and ready for co-author input provide a reasonable review deadline (couple of weeks). Typically, if co-authors are unresponsive its not they don’t find the manuscript important. It is possible that it has been shuffled down the dreaded “to do” list or they just forgot. This has happened to all of us at some point. Therefore a simple reminder would be helpful to re-engage the co-authors. Below is a flow chart from Cooke et al. (2014) with a suggested example of sequence of events to work with tardy or non-responsive co-authors.\n\n\n\n\n\nFlow-diagram with a possile sequence of events for working with tardy or non-responsive co-authors when preparing a manuscript for submission to a journal. From Cooke et al. (2014).\n\n\n\n\nAs suggested above, if co-authors are completely unresponsive it is the lead authors progrative, with input from co-authors to remove unresponsive co-authors. The Committee on Publication Ethics (COPE), has additional guidance information regarding authorship and ethics."
  },
  {
    "objectID": "Authorship.html#diversity-of-contributions",
    "href": "Authorship.html#diversity-of-contributions",
    "title": "Authorship Agreement",
    "section": "",
    "text": "Journals are increasingly requiring authors to specify roles according to author contributions. The days of honorary authorship is slowly (too slowly) coming to an end. However, there are many other aspects of research that are not adequately captured by the generally recognized short list of contributing roles, particularly in applied environmental disciplines such as conservation science, environmental science and applied ecology (herein referred to as conservation science). Cooke et al. (2021) provides some additional context and consideration to authorship guidelines reviewing existing and proposing potential expanded contributors roles.\nBottom line\nPoints summarized from the abstract of Cook et al. (2021).\n\nAuthorship should acknowledge and reward those deserving of credit. Being an author on a paper also means that one assumes some degree of ownership of the content, regardless of the authorship order. (Something, it feels, has been forgotten recently)\nThe growing recognition that authorship should reflect contributions that extend beyond the usual data collection, analysis and writing provides the ideal backdrop for rethinking contributions in conservation science. Cook et al. (2021) propose a more inclusive approach to authorship that recognizes and values diverse contributions and contributors using an expanded list of defined roles."
  },
  {
    "objectID": "Authorship.html#reference",
    "href": "Authorship.html#reference",
    "title": "Authorship Agreement",
    "section": "",
    "text": "Cooke SJ, Donaldson MR, Clark TD (2014) Practical guidance for early career researchers dealing with tardy or unresponsive co-authors. Ideas in Ecology and Evolution. 1(7). Link.\nCooke SJ, Nguyen VM, Young N, et al (2021) Contemporary authorship guidelines fail to recognize diverse contributions in conservation science research. Ecological Solutions and Evidence 2:e12060. Link.\nSaltmarsh Habitat & Avian Research Program (2014) Guidelines for authorship. Saltmarsh Habitat & Avian Research Program. https://www.tidalmarshbirds.org/."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Time series Analysis (Part IV), Stationarity and Peroidicity\n\n\n\n\n\n\n\nblog\n\n\ntime series\n\n\nstationarity\n\n\nperodicity\n\n\n\n\n\n\n\n\n\n\n\nDec 8, 2024\n\n\nPaul Julian\n\n\n\n\n\n\n  \n\n\n\n\nTime series Analysis (Part III), Welcome to the Kendall Family?\n\n\n\n\n\n\n\nblog\n\n\ntime series\n\n\ntrend\n\n\n\n\n\n\n\n\n\n\n\nNov 17, 2024\n\n\nPaul Julian\n\n\n\n\n\n\n  \n\n\n\n\nTime series Analysis (Part II), How Much Data?\n\n\n\n\n\n\n\nblog\n\n\ntime series\n\n\npower\n\n\n\n\n\n\n\n\n\n\n\nNov 3, 2024\n\n\nPaul Julian\n\n\n\n\n\n\n  \n\n\n\n\nTime series Analysis (Part I), the basics\n\n\n\n\n\n\n\nblog\n\n\ntime series\n\n\n\n\n\n\n\n\n\n\n\nOct 26, 2024\n\n\nPaul Julian\n\n\n\n\n\n\n  \n\n\n\n\nPrevious Blog Posts and Migration\n\n\n\n\n\n\n\nblog\n\n\nmigration\n\n\n\n\n\n\n\n\n\n\n\nDec 16, 2023\n\n\nPaul Julian\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "news/2023-12-16-previous-posts.html",
    "href": "news/2023-12-16-previous-posts.html",
    "title": "Previous Blog Posts and Migration",
    "section": "",
    "text": "As I migrate posts from 2021-2022, please find them here on the prior blog page\nhttps://swampthingecology.org/blog/"
  },
  {
    "objectID": "news/2023-12-16-previous-posts.html#migrating-previous-posts",
    "href": "news/2023-12-16-previous-posts.html#migrating-previous-posts",
    "title": "Previous Blog Posts and Migration",
    "section": "",
    "text": "As I migrate posts from 2021-2022, please find them here on the prior blog page\nhttps://swampthingecology.org/blog/"
  },
  {
    "objectID": "news/20241103_Timeseries2.html",
    "href": "news/20241103_Timeseries2.html",
    "title": "Time series Analysis (Part II), How Much Data?",
    "section": "",
    "text": "In my last post I gave a relatively high level overview of time series and some basic analyses. Next we are going to talk about how much data is needed to perform some of these basic tests. Not necessarily the ins and outs (i.e. assumptions) of the statistical test(s) but how much data is needed to (1) run the test and (2) get a meaningful result. Most of this will be a discussion (with exercises) on statistical power. We will be also bring back our synthetic data functions introduced in the last post to provide some context. Most of the discussion will be centered around the Kendall family of trend tests."
  },
  {
    "objectID": "news/20241103_Timeseries2.html#minimum-number-of-samples",
    "href": "news/20241103_Timeseries2.html#minimum-number-of-samples",
    "title": "Time series Analysis (Part II), How Much Data?",
    "section": "Minimum number of samples",
    "text": "Minimum number of samples\nI often get asked how many samples do I need or what criterion did you use to screen the dataset to perform the trend test. Technically you only need two points to draw a line, but with those two points you don’t have the ability to understand the uncertainty (i.e. variance) of said line.\nWhats the absolute minimum needed to run the test? Most trend tests require 2 - 3 values to just do the math. Lets take the Kendall correlation as an example\n\n\n\nset.seed(123)\nyval2 &lt;- rnorm(2)*0.5\nxval2 &lt;- 1:length(yval2)\ncor.test(yval2,xval2,method=\"kendall\")\n\n\n    Kendall's rank correlation tau\n\ndata:  yval2 and xval2\nT = 1, p-value = 1\nalternative hypothesis: true tau is not equal to 0\nsample estimates:\ntau \n  1 \n\n\n\nset.seed(123)\nyval3 &lt;- rnorm(3)*0.5\nxval3 &lt;- 1:length(yval3)\ncor.test(yval3,xval3,method=\"kendall\")\n\n\n    Kendall's rank correlation tau\n\ndata:  yval3 and xval3\nT = 3, p-value = 0.3333\nalternative hypothesis: true tau is not equal to 0\nsample estimates:\ntau \n  1 \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat does this mean? For the Mann-Kendall test, the test statistic \\(S\\) is calculated as the sum of signs of differences between all pairs of observations therefore you only technically need two points to perform the test … but again this lacks any certainty and does not have a robust amount of data to accurately estimate the statistical likelihood. We can do a thought experiment to look at the number of samples to calculate the exact critical values for the \\(S\\) statistic. This can be achieved by calculating all possible \\(S\\) values for a given sample size \\(n\\) and determining the probabilities of obtaining these values under the null hypothesis of no trend.\nHere is the basic functions needed to generate the critical value look-up table for the Mann-Kendall \\(S\\) statistic.\n\ncalculate_S: This function calculates the Mann-Kendall \\(S\\)-Statistic for a given vector (x). It loops over all pairs \\((i,j)\\) to compute the sum of signs. In the correlation test examples above this is the T statistic.\ncalculate_Smax: This function calculates the \\(S_{max}\\) value for a given sample size (\\(n\\)).\ngenerate_lookup_table: This function generates all possible permutations of ranks for a given sample size \\(n\\), calculates \\(S\\) for each permutation, and then tabulates the frequency of each unique \\(S\\) value. It calculates the probability of each \\(S\\)-value by dividing its frequency by the total number of permutations.\n\n\nlibrary(gtools) # for permutations\n\n# Define function to calculate Mann-Kendall S statistic\ncalculate_S &lt;- function(x) {\n  n &lt;- length(x)\n  S &lt;- 0\n  for (i in 1:(n - 1)) {\n    for (j in (i + 1):n) {\n      S &lt;- S + sign(x[j] - x[i])\n    }\n  }\n  return(S)\n}\n\ncalculate_Smax &lt;- function(n){n*(n-1)/2}\n\n# Function to generate look-up table for a given sample size n\ngenerate_lookup_table &lt;- function(n) {\n  # Generate all possible permutations of ranks\n  rank_permutations &lt;- permutations(n, n, v = 1:n)\n  \n  # Calculate S for each permutation\n  S_values &lt;- apply(rank_permutations, 1, calculate_S)\n  \n  # Count frequency of each S value\n  S_table &lt;- table(S_values)\n  \n  # Calculate probabilities\n  total_permutations &lt;- factorial(n)\n  prob_table &lt;- data.frame(\n    S = as.numeric(names(S_table)),\n    Frequency = as.integer(S_table),\n    # Smax = as.integer(n*(n-1)/2),\n    Probability = as.integer(S_table) / total_permutations\n  )\n  \n  return(prob_table)\n}\n\nLets look at samples sizes of 2, 3, 4 and 5. Any more than that and the process get bogged down (due to the computation of individual permutations). Lets use the lapply function to generate look up tables for a range of n values.\n\n# Generate look-up tables for sample sizes n = 2, 3, 4, 5 and 10\nlookup_tables &lt;- lapply(2:5, generate_lookup_table)\nsmax_val &lt;- calculate_Smax(2:5)\nnames(lookup_tables) &lt;- paste0(\"n=\", 2:5,\"; Smax = \",smax_val)\n\n# Display the look-up tables\nlookup_tables\n\n$`n=2; Smax = 1`\n   S Frequency Probability\n1 -1         1         0.5\n2  1         1         0.5\n\n$`n=3; Smax = 3`\n   S Frequency Probability\n1 -3         1   0.1666667\n2 -1         2   0.3333333\n3  1         2   0.3333333\n4  3         1   0.1666667\n\n$`n=4; Smax = 6`\n   S Frequency Probability\n1 -6         1  0.04166667\n2 -4         3  0.12500000\n3 -2         5  0.20833333\n4  0         6  0.25000000\n5  2         5  0.20833333\n6  4         3  0.12500000\n7  6         1  0.04166667\n\n$`n=5; Smax = 10`\n     S Frequency Probability\n1  -10         1 0.008333333\n2   -8         4 0.033333333\n3   -6         9 0.075000000\n4   -4        15 0.125000000\n5   -2        20 0.166666667\n6    0        22 0.183333333\n7    2        20 0.166666667\n8    4        15 0.125000000\n9    6         9 0.075000000\n10   8         4 0.033333333\n11  10         1 0.008333333\n\n\nEach look-up table contains:\n\nS: The possible Mann-Kendall statics values\nFrequency: The frequency of each \\(S\\)-value among all permutations\nProbability: The probability of obtaining each S-value under the null hypothesis.\nSmax: The maximum possible value of \\(S\\) for a given sample size. \\(S_{max}\\) is useful for determining critical values and conducting hypothesis tests in the Mann-Kendall trend test, particularly when assessing the significance of the observed \\(S\\)-statistic under the null hypothesis of no trend.\n\nAs you can see a sample size of two (n=2) you have a an equal chance (probability) of accepting or rejecting your hypothesis.\nWhile this section pertains to how few samples are needed to do the test, its worth mentioning the other side of the coin. When sample sizes increase some of the mathematics changes slightly. For instance when \\(n\\) is large (a relative term … lets just say \\(n\\ge10\\)), the standardized \\(S\\) (often denoted as \\(Z\\)) approximately follows a normal distribution. This allows the use of z-scores to determine statistical significance of the observed \\(S\\)-statistic.\nNow that we know the fundamentals of two points make a straight line … we can move onto statistical power."
  },
  {
    "objectID": "news/20241103_Timeseries2.html#how-to-do-a-power-analysis",
    "href": "news/20241103_Timeseries2.html#how-to-do-a-power-analysis",
    "title": "Time series Analysis (Part II), How Much Data?",
    "section": "How to do a power analysis",
    "text": "How to do a power analysis\nStatistical power analysis can be performed a couple different ways. For this post, lets assume this is a post-hoc statistical power analysis (remember you can do as power analysis before collecting samples to determine how many samples you need). In this example we will use a power calculation for a general linear model to approximate the power. The reason for the caveat is a general linear model is NOT a trend test (unless the data and model residuals fit the assumptions of the test … fat chance) and is different from the the Mann-Kendall and Thiel-Sen estimator.\n\npwr\nThe pwr.f2.test(...) function in the pwr library calculates the statistical power for a linear model. Its pretty straight forward …\n\nlibrary(pwr)\n\n# Define parameters\neffect_size &lt;- 0.15  # Estimate of the effect size (can be calculated or approximated)\nalpha &lt;- 0.05        # Significance level\npower &lt;- 0.8         # Desired power\n\n# Perform power analysis for linear regression\nsample_size &lt;- pwr.f2.test(u = 1, f2 = effect_size, sig.level = alpha, power = power)\n\n# Output the required number of samples\nsample_size\n\n\n     Multiple regression power calculation \n\n              u = 1\n              v = 52.315\n             f2 = 0.15\n      sig.level = 0.05\n          power = 0.8\n\n\n\nu = 1 represents one predictor variable (time, in this case).\nf2 is the effect size (Cohen’s \\(f^{2}\\)) for linear models and is calculated using the models \\(R_{2}\\).\n\n\n\\(f^{2} = \\frac{R^{2}}{1 - R^{2}}\\)\n\nOf course knowing the effect size and power is half the information we want to know. Another way to do a power analysis and probably a more meaningful way would be a simulation based test. I’ll show you two different versions, a very basic version and a more elaborate version based off an analysis presented by Dr Schramm  in this paper.\n\n\nBasic Simulation\n\nset.seed(123)\n# Parameters\ntrue_slope &lt;- 0.05  # Slope of the trend\nsigma &lt;- 1          # Standard deviation of errors\nn_sim &lt;- 1000       # Number of simulations\nsample_sizes &lt;- seq(20, 100, by = 5)  # Range of sample sizes to test\nalpha &lt;- 0.05\n\n# Function to simulate data and fit regression model\npower_simulation &lt;- function(n) {\n  significant_results &lt;- 0\n  for (i in 1:n_sim) {\n    x &lt;- 1:n\n    y &lt;- true_slope * x + rnorm(n, mean = 0, sd = sigma)\n    model &lt;- lm(y ~ x)\n    if (summary(model)$coefficients[2, 4] &lt; alpha) {  # p-value of slope\n      significant_results &lt;- significant_results + 1\n    }\n  }\n  return(significant_results / n_sim)\n}\n\n# Run simulations for each sample size\npower_results &lt;- sapply(sample_sizes, power_simulation)\n\n\n\n\n\n\nStatistical power by sample size.\n\n\n\n\nA power of 0.80 is typically considered appropriate, which equates to a 20% chance of encountering a Type II error. Remember from above the definition of power, if we are sampling from a population where the null hypothesis is false, the power is calculated as\n\n\\[ Power = \\frac{N_{rejected}{N}\\]\n\nwhere \\(N\\) is the total number of tests and \\(N_{rejected}\\) are the total number of times the test rejected the null hypothesis. I highly recommend reading this paper for a great IRL example of estimating statistical power in trend analyses. I’ve personally used variants of this but they haven’t officially made it into my peer-reviewed pubs yet.\n\n\nSimulation Based Power Analysis\nSimilar to our our dat2 time series from the prior post we are adding in some extra noise.\n\nn.yrs &lt;- 20\nn.vals &lt;- n.yrs*12 \n\ndat &lt;- simulate_timeseries_vol(\n  n = n.vals,           # About 20 years of daily data\n  trend_slope = 0.05,   # Upward trend\n  seasonal_amp = 2,     # Base seasonal amplitude\n  seasonal_period = 12, # Monthly seasonality\n  init_vol = 0.5,      # Initial volatility\n  vol_persistence = 0.65,\n  rw_sd = 0.3\n)\n## add some years and months\ndat$timeseries  &lt;-  cbind(dat$timeseries,\n                           expand.grid(Mon = 1:12, Yr = 1990:(1990+(n.yrs-1)))\n)\ndat$timeseries$date &lt;-  with(dat$timeseries,as.Date(paste(Yr,Mon,\"01\",sep=\"-\")))\n\n\n\n\n\n\nThe simulated time series.\n\n\n\n\nLets aggregate the data to year (Yr) for this example by calculating the annual mean\n\nlibrary(plyr)\ndat_yr &lt;- ddply(dat$timeseries,c(\"Yr\"),summarise,\n                mean.val = mean(value),\n                sd.val = sd(value)\n                )\n\n\n\n\n\n\nQuick plots of annually aggregated data.\n\n\n\n\nMuch like the basic simulation power analysis presented about this method will also use a simulation.\n\nlibrary(mblm)\n\nyrs &lt;- seq(min(dat_yr$Yr),max(dat_yr$Yr))\nyrs2 &lt;- yrs[3:length(yrs)]\n\nnsims &lt;- 1000\npvalues &lt;- NA\npower.trend.detect &lt;- data.frame()\nset.seed(123)\nfor(i in 1:length(yrs2)){\n  \n  tmp.dat &lt;- subset(dat_yr,Yr%in%seq(yrs[1],yrs2[i]))\n  \n  for (j in 1:nsims) {\n    yval &lt;- rnorm(n=length(tmp.dat$mean.val),\n                  mean=tmp.dat$mean.val,\n                  sd=sd(tmp.dat$mean.val))\n    trend.test &lt;- with(tmp.dat,\n                       cor.test(yval,Yr,method=\"kendall\"))\n    pvalues[j] &lt;-  trend.test$p.value\n  }\n  \n  thiel_sen  &lt;-  mblm(mean.val~Yr,tmp.dat)\n  trend &lt;- with(tmp.dat,cor.test(mean.val,Yr,method=\"kendall\"))\n  \n  power  &lt;-  sum(pvalues &lt; 0.05)/nsims\n  rslt  &lt;- data.frame(Yr=yrs2[i],\n                      slope = as.numeric(coefficients(thiel_sen)[2]),\n                      kendall.tau = as.numeric(trend$estimate),\n                      kendall.pval = as.numeric(trend$p.value),\n                      trend.power=power)\n  power.trend.detect  &lt;- rbind(power.trend.detect,rslt)\n}\npower.trend.detect$yrs &lt;- power.trend.detect$Yr-min(dat_yr$Yr)\n\npower.trend.detect\n\n     Yr     slope kendall.tau kendall.pval trend.power yrs\n1  1992 0.3503085   0.3333333 1.000000e+00       0.000   2\n2  1993 0.5555924   0.6666667 3.333333e-01       0.000   3\n3  1994 0.7112633   0.8000000 8.333333e-02       0.096   4\n4  1995 0.7071783   0.8666667 1.666667e-02       0.161   5\n5  1996 0.6867529   0.9047619 2.777778e-03       0.290   6\n6  1997 0.7071783   0.9285714 3.968254e-04       0.411   7\n7  1998 0.7596028   0.9444444 4.960317e-05       0.549   8\n8  1999 0.7071783   0.9111111 2.976190e-05       0.620   9\n9  2000 0.6916715   0.8545455 4.624619e-05       0.643  10\n10 2001 0.6722881   0.8484848 1.634233e-05       0.742  11\n11 2002 0.6712462   0.7179487 2.839785e-04       0.616  12\n12 2003 0.6504437   0.6483516 7.575015e-04       0.620  13\n13 2004 0.5643232   0.6571429 3.303709e-04       0.688  14\n14 2005 0.5611362   0.6833333 8.266192e-05       0.721  15\n15 2006 0.4400709   0.7205882 1.148789e-05       0.750  16\n16 2007 0.4299183   0.7516340 1.442393e-06       0.856  17\n17 2008 0.4121871   0.7777778 1.649783e-07       0.878  18\n18 2009 0.4167471   0.8000000 1.731000e-08       0.926  19\n\n\n\n\n\n\n\nSimulation based power analysis of annual trend on the dat dataset using a normal distribution.\n\n\n\n\nBased on this analysis, due to the variability in the data it took almost 18 simulated years of data before it reached a power of ≥ 0.80. Meanwhile, we didn’t see a “significant” trend until year five despite a relative low “power”. As more years were added, the power increased. Now this is based on annually aggregated data and an annual trend test. This could also be done on a seasonal Kendall trend test, which probably show different results given the seasonal nature of the data. We can also tune the knobs on the synthetic data function to see how things change given all the factors.\nThis analysis was done by simulating from a normal distribution, it can also be done with other distributions such as Gamma or even Tweedie. These distributions are great for right skewed data (always positive). Below is an example using the tweedie to perform the power simulations, its a litte more involved to simulate around a given mean due to the distributions parameters and how variance is estimated. From a tweedie distribution variance is estimates as:\n\n\\[Var(Y) = \\phi \\cdot \\mu^{p}\\]\n\nWhere:\n\n\\(\\phi\\) is the dispersion parameter\n\\(\\mu\\) is the mean of the distribution\n\\(p\\) is the Tweedie power parameter which determines the specific type or distribution (e.g. \\(p=1\\) is a Poisson distribution, \\(p=2\\) is a Gamma distribution and \\(p=3\\) is a Inverse Gaussian distribution)\n\nTo determine the standard deviation (what we usually plug into things like rnorm(...)) you take the square root of the variance:\n\n\\[\\sigma_{Y} = \\sqrt{\\phi \\cdot \\mu^{p}}\\]\n\nTo estimate a given standard deviation we need to solve for \\(\\phi\\) in the equation above.\n\n\\[\\phi = \\dfrac{\\sigma_{y}}{\\mu^{p}}\\]\n\nFor purposes of this exercise lets assume a power parameter of \\(p\\) of 1.2. To get a better sense of distribution variables you can use the tweedie.profile function in the tweedie library. First lets double check we can estimate close to the mean and standard deviation we are expecting\n\nlibrary(tweedie)\n\n# Set parameters\nmu &lt;- 2        # Example mean\np &lt;- 1.2       # Example Tweedie power parameter \nsigma_Y &lt;- 1   # the expected standard deviation \nphi &lt;- sigma_Y / (mu^p)  # Calculate phi for a given standard deviation\n\n# Simulate Tweedie data\nset.seed(123)\nsim_data &lt;- rtweedie(n = 1000, mu = mu, phi = phi, power = p)\n\n# Check standard deviation of simulated data\nsd(sim_data)\n\n[1] 0.9900953\n\n# Check mean of simulated data\nmean(sim_data)\n\n[1] 1.976365\n\n\n… close enough for government distributions. Lets replace the rnorm in the power analysis above with rtweedie and go through the process.\n\ntw.p &lt;- 1.2 # Tweedie power parameter\n\npvalues &lt;- NA\npower.trend.detect.tw &lt;- data.frame()\nset.seed(123)\nfor(i in 1:length(yrs2)){\n  \n  tmp.dat &lt;- subset(dat_yr,Yr%in%seq(yrs[1],yrs2[i]))\n  \n  for (j in 1:nsims) {\n    tw.mu &lt;- (tmp.dat$mean.val)\n    tw.phi &lt;- sd(tmp.dat$mean.val) / (tw.mu^tw.p)\n    \n    yval &lt;- rtweedie(n=length(tmp.dat$mean.val),\n                     mu = tw.mu, phi = tw.phi, power = tw.p)\n    trend.test &lt;- with(tmp.dat,\n                       cor.test(yval,Yr,method=\"kendall\"))\n    pvalues[j] &lt;-  trend.test$p.value\n  }\n  \n  thiel_sen  &lt;-  mblm(mean.val~Yr,tmp.dat)\n  trend &lt;- with(tmp.dat,cor.test(mean.val,Yr,method=\"kendall\"))\n  \n  power  &lt;-  sum(pvalues &lt; 0.05)/nsims\n  rslt  &lt;- data.frame(Yr=yrs2[i],\n                      slope = as.numeric(coefficients(thiel_sen)[2]),\n                      kendall.tau = as.numeric(trend$estimate),\n                      kendall.pval = as.numeric(trend$p.value),\n                      trend.power=power)\n  power.trend.detect.tw  &lt;- rbind(power.trend.detect.tw,rslt)\n}\npower.trend.detect.tw$yrs &lt;- power.trend.detect.tw$Yr-min(dat_yr$Yr)\n\npower.trend.detect.tw\n\n     Yr     slope kendall.tau kendall.pval trend.power yrs\n1  1992 0.3503085   0.3333333 1.000000e+00       0.000   2\n2  1993 0.5555924   0.6666667 3.333333e-01       0.000   3\n3  1994 0.7112633   0.8000000 8.333333e-02       0.099   4\n4  1995 0.7071783   0.8666667 1.666667e-02       0.233   5\n5  1996 0.6867529   0.9047619 2.777778e-03       0.494   6\n6  1997 0.7071783   0.9285714 3.968254e-04       0.678   7\n7  1998 0.7596028   0.9444444 4.960317e-05       0.863   8\n8  1999 0.7071783   0.9111111 2.976190e-05       0.917   9\n9  2000 0.6916715   0.8545455 4.624619e-05       0.919  10\n10 2001 0.6722881   0.8484848 1.634233e-05       0.954  11\n11 2002 0.6712462   0.7179487 2.839785e-04       0.889  12\n12 2003 0.6504437   0.6483516 7.575015e-04       0.865  13\n13 2004 0.5643232   0.6571429 3.303709e-04       0.911  14\n14 2005 0.5611362   0.6833333 8.266192e-05       0.934  15\n15 2006 0.4400709   0.7205882 1.148789e-05       0.970  16\n16 2007 0.4299183   0.7516340 1.442393e-06       0.986  17\n17 2008 0.4121871   0.7777778 1.649783e-07       0.993  18\n18 2009 0.4167471   0.8000000 1.731000e-08       0.997  19\n\n\n\n\n\n\n\nSimulation based power analysis of annual trend on the dat dataset using a tweedie distribution.\n\n\n\n\nUsing a Tweedie distribution gives a slightly different evaluation of statistical power than the normal distribution estimate above. As in most cases, distribution matters feel free to adapt this code for whatever distribution your data is in.\nGiven all this trend analyses and power of the test are dependent upon several factors, including getting at the minimum number of samples needed. These factors include, but are not limited to:\n\nTrend Strength: For a “strong” trend, fewer points might suffice, but “weaker” trends require more data points to confidently discern a pattern.\nData Variability: Higher variability in the data means more points are needed to distinguish a trend from random fluctuations.\nSeasonality and Autocorrelation: If there’s seasonality or serial correlation, you may need to account for it or have more data points to ensure the test’s assumptions are met."
  },
  {
    "objectID": "news/20241208_Timeseries4.html",
    "href": "news/20241208_Timeseries4.html",
    "title": "Time series Analysis (Part IV), Stationarity and Peroidicity",
    "section": "",
    "text": "Congrats, you made it to the fourth installment of the time-series analysis blog post series. So far we’ve gone over the basics of time series (Part I), minimum number of samples, power analyses and some basic trend analyses (Part II), and met the family … Kendall family that is (Part III) . In this post we will begin the discussion of time-series stationarity and periodicity, both of which are important considerations in time series forcasts, time-series decomposition and other analyses."
  },
  {
    "objectID": "news/20241208_Timeseries4.html#time-series",
    "href": "news/20241208_Timeseries4.html#time-series",
    "title": "Time series Analysis (Part IV), Stationarity and Peroidicity",
    "section": "Time-series",
    "text": "Time-series\nBefore we dig too deep into the definitions and tests associated with stationarity and periodicity lets simulate a dataset. If you remember in the first post of this series (Part I), I had a couple of functions put together to simulate a time-series.\n\nn.vals &lt;- 15*12 # 15 years 12 months per year \ndat1 &lt;- simulate_timeseries(\n  n = n.vals,           # About 15 years of monthly data\n  trend_slope = 0.025,  # Upward trend\n  seasonal_amp = 2,     # Seasonal fluctuation of ±2\n  seasonal_period = 12, # Monthly seasonality\n  noise_sd = 0.5,       # Moderate noise\n  noise_mean = 3\n)\n## add some years and months\ndat1$timeseries  &lt;-  cbind(dat1$timeseries,\n                           expand.grid(Mon = 1:12, Yr = 1990:2004)\n)\ndat1$timeseries$date &lt;-  with(dat1$timeseries,as.Date(paste(Yr,Mon,\"01\",sep=\"-\")))\n\ndat2 &lt;- simulate_timeseries_vol(\n  n = n.vals,           # About 15 years of daily data\n  trend_slope = 0.05,   # Upward trend\n  seasonal_amp = 2,     # Base seasonal amplitude\n  seasonal_period = 10, # Monthly seasonality\n  init_vol = 0.5,      # Initial volatility\n  vol_persistence = 0.65,\n  rw_sd = 0.3\n)\n## add some years and months\ndat2$timeseries  &lt;-  cbind(dat2$timeseries,\n                           expand.grid(Mon = 1:12, Yr = 1990:2004)\n)\ndat2$timeseries$date &lt;-  with(dat2$timeseries,as.Date(paste(Yr,Mon,\"01\",sep=\"-\")))\n\ndat3 &lt;- simulate_timeseries(\n  n = n.vals,           # About 15 years of monthly data\n  trend_slope = 0.001,  # no trend\n  seasonal_amp = 2,     # Seasonal fluctuation of ±2\n  seasonal_period = 12, # Monthly seasonality\n  noise_sd = 0.5,       # Moderate noise\n  noise_mean = 3\n)\n## add some years and months\ndat3$timeseries  &lt;-  cbind(dat3$timeseries,\n                           expand.grid(Mon = 1:12, Yr = 1990:2004)\n)\ndat3$timeseries$date &lt;-  with(dat3$timeseries,as.Date(paste(Yr,Mon,\"01\",sep=\"-\")))\n\n\n\n\n\n\nQuick plots of example data"
  },
  {
    "objectID": "news/20241208_Timeseries4.html#stationarity",
    "href": "news/20241208_Timeseries4.html#stationarity",
    "title": "Time series Analysis (Part IV), Stationarity and Peroidicity",
    "section": "Stationarity",
    "text": "Stationarity\nYou might see throughout the literature stationary and stationarity, one term is a definition and the other is a status. For instance a stationary time-series has statistical properties or moments that do not vary in time. These proprieties or moments refer to the mean, variance or covariance of a time-series. Therefore a stationary time-series is a constant mean, variance and autocorrelation. Stationarity, is therefore, the status of a time-series. These words and their antonyms (e.g. stationarity vs non-stationarity, stationary vs non-stationary) are scatter throughout the literature and used to represent the same things in one way or another. My word savy friends might be able to settle when or how to uses these variants, for now I’ll try to remain as consistent as possible.\nTo test for stationarity there are several statistical statistical tests and diagnostic tools are available.\n\nInspect the data, I know this seems basic but looking at the data will help you gauge the data qualitatively. See plots above.\nChanges in mean and variance, given the definitions of stationarity looking at the changes in mean and variance over the time-series could provide insight into the structure of the data and any major shifts or changes. To do that we can look at the cumulative change or deviation in mean values, sometimes called the cusum. Another version of this is looking at the cumulative deviation within the dataset. Cusum with respect to mean is pretty straight forward, estimate the mean of a dataset and calculate the cumulative deviation from the mean.\n\n\n## cusum (WRT mean)\ndat1.mu &lt;- mean(dat1$timeseries$value)\ndat1.cusum &lt;- cumsum(dat1$timeseries$value - dat1.mu)\n\ndat2.mu &lt;- mean(dat2$timeseries$value)\ndat2.cusum &lt;- cumsum(dat2$timeseries$value - dat2.mu)\n\ndat3.mu &lt;- mean(dat3$timeseries$value)\ndat3.cusum &lt;- cumsum(dat3$timeseries$value - dat3.mu)\n\n\n\n\n\n\nDeviance and cusum for each dataset\n\n\n\n\nBased on the deviation from mean and cusum plots you can see that there are some similarities and differences between time-series. With dat1 and dat2 given their generally increasing trends the deviation from the overall mean also increases, meanwhile dat3 is relatively constant. Simularly the cusum plots look similar for dat1 and dat2 with a big “U”, however the shape of the U is different between time-series. The bottom of “U” inidicates where in the time-series is the greatest deviation from the overall mean.\nThe cusum approach can also be applied to looking at changes in variance across the time-series. The approach is similar, the overall variance is calculated and compared to a squared deviance.\n\n## cusum (WRT variance)\ndat1.var &lt;- var(dat1$timeseries$value)\ndat1.sqdev &lt;- (dat1$timeseries$value - dat1.mu)^2\ndat1.cusum_variance &lt;- cumsum(dat1.sqdev - dat1.var)\n\ndat2.var &lt;- var(dat2$timeseries$value)\ndat2.sqdev &lt;- (dat2$timeseries$value - dat2.mu)^2\ndat2.cusum_variance &lt;- cumsum(dat2.sqdev - dat2.var)\n\ndat3.var &lt;- var(dat3$timeseries$value)\ndat3.sqdev &lt;- (dat3$timeseries$value - dat3.mu)^2\ndat3.cusum_variance &lt;- cumsum(dat3.sqdev - dat3.var)\n\n\n\n\n\n\nDeviance and cusum for each dataset\n\n\n\n\nChanges in the cusum variance lines gives an indication of changes in variability over time within the time-series. Positive “trend” indicates increasing variability and vice-versa.\n\nAugmented Dickey-Fuller (ADF) Test, the ADF test checks for a unit root in the time-series, where the null hypothesis is that the series is non-stationary\n\n\nlibrary(tseries)\n\nadf.test(dat1$timeseries$value)\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  dat1$timeseries$value\nDickey-Fuller = -14.195, Lag order = 5, p-value = 0.01\nalternative hypothesis: stationary\n\nadf.test(dat2$timeseries$value)\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  dat2$timeseries$value\nDickey-Fuller = -2.502, Lag order = 5, p-value = 0.3664\nalternative hypothesis: stationary\n\nadf.test(dat3$timeseries$value)\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  dat3$timeseries$value\nDickey-Fuller = -14.195, Lag order = 5, p-value = 0.01\nalternative hypothesis: stationary\n\n\nThese results suggest that both dat1 and dat3 are stationary datasets due to the low p-values. To some extent you can see that in the plots of the raw data with dat2 time-series looks like something changes towards the back end of the time-series.\n\nKPSS Test (Kwiatkowski-Philips-Schmidt-Shin), this test complements the ADF test\n\n\nkpss.test(dat1$timeseries$value,null = \"Trend\")\n\n\n    KPSS Test for Trend Stationarity\n\ndata:  dat1$timeseries$value\nKPSS Trend = 0.010153, Truncation lag parameter = 4, p-value = 0.1\n\nkpss.test(dat2$timeseries$value,null = \"Trend\")\n\n\n    KPSS Test for Trend Stationarity\n\ndata:  dat2$timeseries$value\nKPSS Trend = 0.19067, Truncation lag parameter = 4, p-value = 0.0195\n\nkpss.test(dat3$timeseries$value,null = \"Trend\")\n\n\n    KPSS Test for Trend Stationarity\n\ndata:  dat3$timeseries$value\nKPSS Trend = 0.010153, Truncation lag parameter = 4, p-value = 0.1\n\n\nAt first glance you would think these results are giving contradictory results. This is where understanding the null hypothesis comes in handy. With the KPSS test, when p-values are low (i.e. &lt;0.05) they indicate the time-series is non-stationary while high p-values suggest stationary time-series. Therefore the KPSS results are consistent with the ADF results.\nBoth the ADF and KPSS tests assess data for stationarity by evaluating the unit-root) but they differ fundamentally in their hypotheses and objectives.\n\nADF Test\n\nNull hypothesis (\\(H_0\\)): The time series has a unit root (i.e. it is non-stationary).\nGoal: Test for the presence of a unit root (non-stationarity).\n\nKPSS Test\n\nNull hypothesis (\\(H_0\\)): The time series is stationary (trend-stationary or level-stationary).\nGoal: Test for stationarity.\n\n\nThere are ways to make non-stationary time-series stationary by using differencing methods, however in most cases (in my field) these methods remove some of the fundamental information from the dataset and make interpretation and “back”-calculating difficult."
  },
  {
    "objectID": "news/20241208_Timeseries4.html#periodicity",
    "href": "news/20241208_Timeseries4.html#periodicity",
    "title": "Time series Analysis (Part IV), Stationarity and Peroidicity",
    "section": "Periodicity",
    "text": "Periodicity\nPeriodicity can be defined as the frequency at which observations occur in a time-series. As with everything there are a couple different ways to assess periodicity.\n\nAutocorrelation Function (ACF), the most basic look at a time-series, aside from just plotting the data is looking at the autocorrelation of the data. The ACF can reveal periodicity and if there are significant correlations at regular lags then this could qualtatively indicate that time-series has periodic cycles.\n\n\nacf(dat1$timeseries$value)\nacf(dat2$timeseries$value)\nacf(dat3$timeseries$value)\n\n\n\n\n\n\nACF plots of example data\n\n\n\n\nThe ACF plots present the degree of autocorrelation (i.e. correlation coefficient) of the data. The horizontal blue dashed line is the approximate line indicating significant correlations. Based on this first look we can see that each time-series has some degree of periodicity.\n\nFourier Transform / Periodogram A periodogram or spectral density plot will show the relative strength of different frequencies (periodicities) in a given time-series. The spectrum function is in base R and will provide a plot with results “hidden”. If you give it a variable name it will store the output as an object. (i.e. test1 &lt;- spectrum(...)).\n\n\nspectrum(dat1$timeseries$value,main = \"Series: dat1\")\n\n\n\nspectrum(dat2$timeseries$value,main = \"Series: dat1\")\n\n\n\nspectrum(dat3$timeseries$value,main = \"Series: dat1\")\n\n\n\n\nHere is a comparison of the different time-series\n\n\n\n\n\nCombined spectral density estimation\n\n\n\n\nInterestingly, the combined spectral density plot shows that dat1 and dat3 have identical spectral densities. Which makes sense if you go back to how the data was simulated. The only difference in the function was trend_slope value which made dat3 a much more flat time-series.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLomb-Scargle Periodogram, a method similar to fourier transformations, the Lomb-Scargle periodgrams fits a sinusoidal model to each frequency. This method can be applied to time-series with regular and irregular sampling intervals (periodicity).\n\n\nlibrary(lomb)\ndat1.lsp_result &lt;- lsp(dat1$timeseries$value,type= \"period\")\n\n\n\ndat2.lsp_result &lt;- lsp(dat2$timeseries$value,type= \"period\")\n\n\n\ndat3.lsp_result &lt;- lsp(dat3$timeseries$value,type= \"period\")\n\n\n\n\nOther functions in the lomb library can help visualize and extract information like getpeaks(...) and summary(...). There is a lot more to this method and package than we can dedicate here, therefore I encourage exploration of this package and method."
  }
]